---
---

@book{Jurafsky2009,
	abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology - at all levels and with all modern technologies - this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing. -- Book Description from Website.},
	added-at = {2013-04-24T13:46:19.000+0200},
	address = {Upper Saddle River, N.J.},
	author = {Jurafsky, Dan and Martin, James H.},
	biburl = {https://www.bibsonomy.org/bibtex/2fb7fa20679ebb9d69d27d7c9682fd774/lopusz_kdd},
	description = {Speech and Language Processing (2nd Edition): Daniel Jurafsky, James H. Martin: 9780131873216: Amazon.com: Books},
	interhash = {5f4a309a36c3da5e3becbf0ac5d88413},
	intrahash = {fb7fa20679ebb9d69d27d7c9682fd774},
	isbn = {9780131873216 0131873210},
	keywords = {language},
	publisher = {Pearson Prentice Hall},
	refid = {213375806},
	timestamp = {2013-04-24T13:46:19.000+0200},
	title = {Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition},
	url = {http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/ref=pd_bxgy_b_img_y},
	year = 2009
}

@article{Manning2000,
	author = {Manning, Christopher D and Schuetze, Hinrich},
	isbn = {1420085921},
	journal = {Reading},
	keywords = {and algorithm design,chinese computing,k -nearest neighbor algorithm,text categorization},
	title = {{Foundations of Natural Language Processing}},
	year = {2000}
}

@article{Deerwester1990,
	author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
	issn = {10974571},
	journal = {Journal of the American Society for Information Science},
	title = {{Indexing by latent semantic analysis}},
	year = {1990}
}



@inproceedings{Jurafsky1994,
	author    = {Daniel Jurafsky and
	Chuck Wooters and
	Gary N. Tajchman and
	Jonathan Segal and
	Andreas Stolcke and
	Eric Fosler and
	Nelson Morgan},
	title     = {The berkeley restaurant project},
	booktitle = {The 3rd International Conference on Spoken Language Processing, {ICSLP}
	1994, Yokohama, Japan, September 18-22, 1994},
	publisher = {{ISCA}},
	year      = {1994},
	url       = {http://www.isca-speech.org/archive/icslp\_1994/i94\_2139.html},
	timestamp = {Mon, 13 Jul 2020 18:00:09 +0200},
	biburl    = {https://dblp.org/rec/conf/interspeech/JurafskyWTSSFM94.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{NIPS2013_5021,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	booktitle = {Advances in Neural Information Processing Systems 26},
	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
	pages = {3111--3119},
	year = {2013},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@book{firth57synopsis,
	abstract = {Reprinted in:  Palmer, F. R. (ed.) (1968). Selected Papers of J. R. Firth 1952-59, pages 168-205. Longmans, London. },
	added-at = {2008-05-14T00:52:58.000+0200},
	address = {Oxford},
	author = {Firth, J. R.},
	biburl = {https://www.bibsonomy.org/bibtex/25e3d6c72cdd123a638f71886d78f3c1e/brightbyte},
	booktitle = {Studies in Linguistic Analysis (special volume of the Philological Society)},
	interhash = {b4f769667fdd195b4a75f61f6388a52e},
	intrahash = {5e3d6c72cdd123a638f71886d78f3c1e},
	keywords = {classic linguistics meanign relatedness semantic},
	pages = {1-32},
	publisher = {The Philological Society},
	timestamp = {2009-01-23T09:58:50.000+0100},
	title = {A synopsis of linguistic theory 1930-55.},
	volume = {1952-59},
	year = 1957
}

@inproceedings{pennington2014,
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
	biburl = {https://www.bibsonomy.org/bibtex/296ee059fcb49b261f72f843715c3adb9/florianpircher},
	booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
	interhash = {29813227df1eea94efa14c7df2b5553a},
	intrahash = {96ee059fcb49b261f72f843715c3adb9},
	keywords = {final glove semantics syntax thema:sequence_labeling},
	pages = {1532--1543},
	timestamp = {2018-11-27T09:36:25.000+0100},
	title = {GloVe: Global Vectors for Word Representation},
	url = {http://www.aclweb.org/anthology/D14-1162},
	year = 2014
}

@article{bojanowski2016,
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	year = {2016},
	month = {07},
	pages = {},
	title = {Enriching Word Vectors with Subword Information},
	volume = {5},
	journal = {Transactions of the Association for Computational Linguistics},
	doi = {10.1162/tacl_a_00051}
}

@article{bakarov2018,
	author    = {Amir Bakarov},
	title     = {A Survey of Word Embeddings Evaluation Methods},
	journal   = {CoRR},
	volume    = {abs/1801.09536},
	year      = {2018},
	url       = {http://arxiv.org/abs/1801.09536},
	archivePrefix = {arXiv},
	eprint    = {1801.09536},
	timestamp = {Mon, 13 Aug 2018 16:47:20 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1801-09536.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{collobert2011,
	author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	biburl = {https://www.bibsonomy.org/bibtex/205ed99f1d54e33204e6407254ab4379c/tengmf},
	description = {Natural Language Processing (Almost) from Scratch},
	interhash = {c1e968fc1903e842ab3c638cd5ffca61},
	intrahash = {05ed99f1d54e33204e6407254ab4379c},
	issn = {1532-4435},
	issue_date = {11/1/2011},
	journal = {J. Mach. Learn. Res.},
	keywords = {machine-learning neural-networks},
	month = nov,
	numpages = {45},
	pages = {2493--2537},
	publisher = {JMLR.org},
	timestamp = {2017-04-03T04:21:27.000+0200},
	title = {Natural Language Processing (Almost) from Scratch},
	url = {http://dl.acm.org/citation.cfm?id=2078183.2078186},
	volume = 999888,
	year = 2011
}

@inproceedings{Devlin,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@misc{ba2016layer,
	title={Layer Normalization}, 
	author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
	year={2016},
	eprint={1607.06450},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{HeResnet,
	author    = {Kaiming He and
	Xiangyu Zhang and
	Shaoqing Ren and
	Jian Sun},
	title     = {Deep Residual Learning for Image Recognition},
	journal   = {CoRR},
	volume    = {abs/1512.03385},
	year      = {2015},
	url       = {http://arxiv.org/abs/1512.03385},
	eprinttype = {arXiv},
	eprint    = {1512.03385},
	timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Vaswani2017,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, {\{}USA{\}}},
	editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M and Fergus, Rob and Vishwanathan, S V N and Garnett, Roman},
	pages = {6000--6010},
	title = {{Attention is All you Need}},
	url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need},
	year = {2017}
}

@inproceedings{Raffel2016,
	abstract = {We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic "addition" and "multiplication" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks. 1 MODELS FOR SEQUENTIAL DATA Many problems in machine learning are best formulated using sequential data and appropriate models for these tasks must be able to capture temporal dependencies in sequences, potentially of arbitrary length. One such class of models are recurrent neural networks (RNNs), which can be considered a learnable function f whose output h t = f (x t , h t−1) at time t depends on input x t and the model's previous state h t−1. Training of RNNs with backpropagation through time (Werbos, 1990) is hindered by the vanishing and exploding gradient problem (Pascanu et al., 2012; Hochreiter {\&} Schmidhuber, 1997; Bengio et al., 1994), and as a result RNNs are in practice typically only applied in tasks where sequential dependencies span at most hundreds of time steps. Very long sequences can also make training computationally inefficient due to the fact that RNNs must be evaluated sequentially and cannot be fully parallelized. 1.1 ATTENTION A recently proposed method for easier modeling of long-term dependencies is "attention". Attention mechanisms allow for a more direct dependence between the state of the model at different points in time. Following the definition from (Bahdanau et al., 2014), given a model which produces a hidden state h t at each time step, attention-based models compute a "context" vector c t as the weighted mean of the state sequence h by c t = T j=1 $\alpha$ tj h j where T is the total number of time steps in the input sequence and $\alpha$ tj is a weight computed at each time step t for each state h j. These context vectors are then used to compute a new state sequence s, where s t depends on s t−1 , c t and the model's output at t − 1. The weightings $\alpha$ tj are then computed by e tj = a(s t−1 , h j), $\alpha$ tj = exp(e tj) T k=1 exp(e tk) where a is a learned function which can be thought of as computing a scalar importance value for h j given the value of h j and the previous state s t−1. This formulation allows the new state sequence s to have more direct access to the entire state sequence h. Attention-based RNNs have proven effective in a variety of sequence transduction tasks, including machine translation (Bahdanau et al., 2014), image captioning (Xu et al., 2015), and speech recognition (Chan et al., 2015; Bahdanau et al., 2015). Attention can be seen as analogous to the "soft addressing" mechanisms of the recently proposed Neural Turing Machine (Graves et al., 2014) and End-To-End Memory Network (Sukhbaatar et al., 2015) models.},
	title={Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems}, 
	archivePrefix = {arXiv},
	arxivId = {1512.08756v5},
	author = {Raffel, Colin and Ellis, Daniel P W},
	eprint = {1512.08756v5},
	booktitle = {Workshop track-ICLR 2016 FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS.pdf:pdf},
	year = {2016}
}

@inproceedings{Bahdanau2015a,
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	archivePrefix = {arXiv},
	arxivId = {1409.0473},
	author = {Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
	booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
	eprint = {1409.0473},
	publisher = {International Conference on Learning Representations, ICLR},
	title = {{Neural machine translation by jointly learning to align and translate}},
	year = {2015}
}

@misc{cho2014learning,
	abstract = {In this paper, we propose a novel neural network model called RNN
	Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN
	encodes a sequence of symbols into a fixed-length vector representation, and
	the other decodes the representation into another sequence of symbols. The
	encoder and decoder of the proposed model are jointly trained to maximize the
	conditional probability of a target sequence given a source sequence. The
	performance of a statistical machine translation system is empirically found to
	improve by using the conditional probabilities of phrase pairs computed by the
	RNN Encoder-Decoder as an additional feature in the existing log-linear model.
	Qualitatively, we show that the proposed model learns a semantically and
	syntactically meaningful representation of linguistic phrases.},
	added-at = {2020-03-08T11:41:02.000+0100},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	biburl = {https://www.bibsonomy.org/bibtex/21b65748e52783133211bce750d8e0361/stdiff},
	description = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
	interhash = {a4bf56db9d1f80d8681c1b47de0569b3},
	intrahash = {1b65748e52783133211bce750d8e0361},
	keywords = {neural-network},
	note = {cite arxiv:1406.1078Comment: EMNLP 2014},
	timestamp = {2020-03-08T11:41:02.000+0100},
	title = {Learning Phrase Representations using RNN Encoder-Decoder for
	Statistical Machine Translation},
	url = {http://arxiv.org/abs/1406.1078},
	year = 2014
}

@inproceedings{Sutskever2014,
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	archivePrefix = {arXiv},
	arxivId = {1409.3215},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	booktitle = {Advances in Neural Information Processing Systems},
	eprint = {1409.3215},
	issn = {10495258},
	number = {January},
	pages = {3104--3112},
	publisher = {Neural information processing systems foundation},
	title = {{Sequence to sequence learning with neural networks}},
	volume = {4},
	year = {2014}
}
