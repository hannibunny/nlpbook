{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document models and similarity\n",
    "\n",
    "- Author:      Johannes Maucher\n",
    "- Last update: 05.11.2020\n",
    "\n",
    "This notebook demonstrates how documents can be described in a vector space model. Applying this type of model\n",
    "\n",
    "1. similarities between documents \n",
    "2. similarities between documents and a query \n",
    "\n",
    "can easily be calculated.\n",
    "\n",
    "## Read documents from a textfile \n",
    "\n",
    "It is assumed that a set of documents is stored in a textfile, as e.g. in [MultiNewsFeeds2014-09-12.txt](../Data/MultiNewsFeeds2014-09-12.txt). The individual documents are separated by line-break. In this case the documents can be assigned to the list _listOfNews_ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Kommunikation: Gut kommunizieren macht glücklich    Wie kaum ein anderer hat der Psychologe Friedemann Schulz von Thun untersucht, was Kommunikation ausmacht. In seinem neuen Buch entwickelt er eine Quintessenz davon.\n",
      "Ukraine-Krise: Bundesregierung toleriert ukrainischen Mauerbau    Im Kanzleramt hat man Verständnis für die Pläne der ukrainischen Regierung, eine Mauer entlang der Grenze zu Russland zu bauen. Dies sei allein Entscheidung der Ukraine.\n",
      "Radfahren: Eine Deutsche bringt die Chinesen aufs Fixie    Einst war Peking Fahrrad-Welthauptstadt. Dann kamen die Autos. Ines Brunn kämpft dagegen: Die Inhaberin eines Fixie-Shops will die Chinesen wieder aufs Rad bringen.\n",
      "Zurück in die Wildnis : Jenseits allen Komforts    Drei Jahre lang suchte der Fotograf Antoine Bruy Aussteiger in Europa. Seine Bilder zeigen Menschen, die der Konsum- und Leistungsgesellschaft den Rücken gekehrt haben.\n",
      "Antisemitismus: Zentralrat beklagt \"Schockwellen von Judenhass\"    Judenfeindliche Hetze wie bei Protesten gegen Israel haben die Juden tief getroffen, sagte der Vorsitzende des Zentralrats. Eine Kundgebung soll ein Zeichen setzen.\n",
      "Artenschutz: Diese Haie muss der Mensch nun besser schützen    Ihr Fleisch ist begehrt und wer sie künftig jagt, könnte mit Strafe rechnen: Fünf Haiarten werden auf die Rote Liste des internationalen Artenschutzabkommens gesetzt.\n",
      "Freihandelsabkommen: CETA ist nicht zustimmungsfähig    Das Freihandelsabkommen zwischen Kanada und der EU hebelt demokratische Kontrolle aus. Die TTIP-Blaupause bevorzugt Wirtschaftsinteressen. Dagegen sollte geklagt werden.\n",
      "Afrika: Libyen muss schnell auf die europäische Agenda    Frankreich will einen Militäreinsatz, um Libyen zu stabilisieren. Man muss nicht zustimmen, aber wenigstens hat das verschämte Schweigen zum Kollaps des Landes ein Ende.\n",
      "Hamburger Kunstverein: \"Ich mag Dekoration nicht\"    Bettina Steinbrügge ist Direktorin des Hamburger Kunstvereins. Im Gespräch berichtet sie von ihrer Arbeit und dass sie persönlich ein Freund von Klarheit und Ordnung ist.\n",
      "Digitale Familie : \"Kuss, Mutter\"    Von Fortschrittsverweigerung keine Spur: Auch die Großeltern haben das Internet für sich entdeckt und tragen dort ein digitales Wettrüsten der Silver Surfer aus.\n",
      "Russland-Sanktionen: Rosneft in Schwierigkeiten    Russlands Ölkonzerne brauchen dringend Geld und Know-how aus dem Westen, um ihre Fördermethoden zu modernisieren. Sanktionen treffen sie hart – und damit das ganze Land.\n",
      "Bundesliga-Vorschau: Sensationelle Neuigkeiten von Schweinsteiger    Alle reden nur noch über dieses eine neue Bild von Bastian Schweinsteiger. Und der Menschenrechtler Rummenigge beschützt uns vor Rassismus. Alles Wichtige zum 3. Spieltag\n",
      "Radio Doria: Liefers liebt Pathos    Seine Frau singt, sein \"Tatort\"-Kollege auch. Jetzt veröffentlicht der Schauspieler Jan Josef Liefers ein neues Album mit seiner Band Radio Doria. Muss das denn sein?\n",
      "PKW-Maut: Dobrindt legt Maut-Gesetz im Oktober vor    Der Verkehrsminister kämpft weiter für seine Maut-Pläne. Er kündigte einen Termin für den Gesetzentwurf an. Dann sei auch Gelegenheit für Diskussionen, sagte Dobrindt.\n",
      "Interaktiv: So viel verdienen unsere Abgeordneten nebenher  Peter Gauweiler, Gregor Gysi, Peer Steinbrück: Jeder vierte Bundestagsabgeordnete verdient sich mit Nebentätigkeiten teils erkleckliche Summen dazu. Mancher wird dabei zum Millionär. Mit dem ständig aktualisierten FAZ.NET-Monitor können Sie ab sofort sehen, welcher Abgeordneter wie viel Geld nebenher verdient.\n",
      "Ukraine-Krise: Lesen Sie Putins Stellenbeschreibung  Vor der Höhle des russischen Bären ist ein Sicherheitsabstand einzuhalten, sonst greift er an. Dennoch braucht Europa Russland – und die Ukraine eine Finnlandisierung.\n",
      "Brief an Netanjahu: Israelische Eliteeinheit will Einsätze verweigern  In einem Brief an Ministerpräsident Netanjahu und die israelische Armeeführung kündigen 43 Mitglieder einer Sonderheit des militärischen Geheimdienstes an, künftig Einsätze zu verweigern. Ihnen geht das grenzenlose Ausspionieren der Palästinenser zu weit.\n",
      "Zwanzig Jahre Zeitung im Internet: Schafft den Online-Journalismus ab  Im Herbst 1994 gingen die ersten Medien online. Die Leser haben seitdem alles bekommen, was sie sich nie vorgestellt hatten. Jetzt müssen sie lernen, mit dieser Informationsvielfalt richtig umzugehen.\n",
      "Der unbekannte Ökostrom-Riese China  China gilt nicht gerade als Vorreiter in Sachen Umwelt- und Klimaschutz. Dabei erleben die erneuerbaren Energien im Reich der Mitte gerade einen unglaublichen Boom.\n",
      "Biografie über den Marquis de Sade: Der Skandalöseste aller Skandalösen  Im Vergleich zu seinen Texten liest sich „Shades of Grey“ wie ein Pixi-Buch: Volker Reinhardt gelingt es in seiner Biographie dennoch, sich dem Marquis de Sade nüchtern anzunähern.\n",
      "Nude-Porträts: Ein Fotograf zieht sich aus  Trevor Christensen macht Nackt-Porträts. Allerdings behalten die Porträtierten dabei ihre Kleider an. Nackt ist hier nur der Fotograf selbst. Warum er das macht, erzählt er im Interview.\n",
      "De Maizière verkündet Verbot des „Islamischen Staats“  Bundesinnenminister de Maizière hat „mit sofortiger Wirkung“ Aktivitäten der Terrorgruppe „Islamischer Staat“ verboten. Der Dschihad gefährde Deutschlands Sicherheit und dürfe nicht „durch die Straßen“ getragen werden, sagte de Maizière.\n",
      "Gefallener Superstar Pistorius: „Kein Platz mehr in der paralympischen Bewegung“  Er war der Held des Behindertensports, nun ist er ein verurteilter Straftäter. Ob Oscar Pistorius nach Verbüßen seiner Strafe wieder an Paralympischen Spielen teilnehmen, darüber scheiden sich die Geister.\n",
      "Wen die EU mit den neuen Sanktionen trifft  Die neuen Strafmaßnahmen der EU sind in Kraft. Sie treffen russische Rohstoffriesen, Separatisten-Anführer und einen engen Freund von Staatspräsident Wladimir Putin.\n",
      "Fraktur: Lieber Hybridschweine als Hybridkriege  Putins Schnitzeljagd: Warum ist uns bloß nie aufgefallen, dass die Krim die Form eines Koteletts hat?\n",
      "Wann lohnt sich die Reparatur der Windschutzscheibe?  Hoffnungsfroh von der Werbung eingestimmt, bringt mancher sein Auto zum Spezialisten, damit der seine Windschutzscheibe vom Steinschlag heile. Manchmal geht das, aber oft bleibt nur der Austausch.\n",
      "TV-Kritik „Maybrit Illner“: Der neue Freizeitsozialismus  An Jeremy Rifkins Buch „Die Null-Grenzkosten-Gesellschaft“ kommt auch die Bundesregierung nicht vorbei. Bei Maybrit Illner zeigt Kanzleramtschef Altmaier im Gespräch mit dem Autor, wie drängend das Thema und wie ideenlos die Politik ist.\n",
      "Ausspähprogramm „Prism“: Wie die amerikanische Regierung Yahoo drohte  Gewährt uns Zugang zu Nutzerdaten oder zahlt 250.000 Dollar Bußgeld pro Tag – mit dieser Forderung soll die amerikanische Regierung im Jahr 2008 Yahoo unter Druck gesetzt haben. Bedenken des Internetkonzerns wischten Gerichte angeblich beiseite.\n",
      "Gesundheitssystem: Mit bestem Dank an  die Privatpatienten  Ein früherer Termin beim Facharzt? Ein ausführlicheres Arztgespräch? Kein Problem für Privatpatienten. Für sie haben Ärzte mehr Zeit. Gesetzlich Versicherte ärgert das. Doch ohne die Privatversicherungen müssten viele Praxen schließen.\n",
      "Zum Tod von Joachim Fuchsberger: Der letzte deutsche Fernseh-Altmeister  Moderator, Schauspieler, Schlagertexter - Joachim Fuchsberger war ein Tausendsassa, der die deutsche Fernsehgeschichte prägte. In Männern wie ihm erkennen wir unser Land.\n",
      "Ukraine-Krise: Zweifelhafte Majdan-Kämpfer  Die Berichte über Verbrechen der Separatisten in der Ostukraine sind fast schon Routine geworden. Nun häufen sich Nachrichten, dass Schuld und Verstrickung nicht mehr nur einer einzigen Seite anzulasten sind.\n",
      "Das Trikot und der Lohn: 15 Cent für vier Sterne  Entwicklungsminister Müller kritisiert das Geschäft mit dem DFB-Trikot. Der Vorwurf sei falsch, sagt Adidas. Für Claudia Roth sind die Fertigungsumstände „unerträglich“.\n",
      "Gabriele Pauli auf Sylt: Reif für die Insel  Die frühere fränkische Landrätin Gabriele Pauli, die einst am Sturz Stoibers mitwirkte, strebt das Bürgermeisteramt auf Sylt an. Nun sammelt sie in Westerland Unterschriften für ihre Kandidatur.\n",
      "Flüchtlinge aus der Ostukraine: Kostenlos nach Sibirien  Russland verspricht Übersiedlern aus der Ostukraine Arbeit und ein Dach über dem Kopf. Doch nach der Landung gibt es für die Flüchtlinge oft ein böses Erwachen.\n",
      "Artikel in Listenform: Die Qual der Zahl  Im Netz verbreiten sich Kurzartikel, die ihre Leser mit „10 guten Gründen“ oder „15 unglaublichen Fakten“ überzeugen wollen. Häufig findet man diese Posts bei Buzzfeed – bald auch in deutscher Sprache.\n",
      "100 Jahre Marne-Schlacht: Verloren im Nebel des Krieges  Wenige Wochen nach dem Ausbruch des Ersten Weltkrieges kam der Vormarsch der deutschen Armeen in Frankreich an der Marne zum Stillstand. Doch selbst bei einem Erfolg in der Marne-Schlacht wäre ein Zweifrontenkrieg nicht mehr zu vermeiden gewesen.\n",
      "Neue Optik für Scheine: Jetzt kommen die neuen Zehner  In zwei Wochen kommen die neuen Zehn-Euro-Scheine. Die Fehler der Fünfer-Umstellung sollen sich nicht wiederholen. Fahrkartenautomaten sollen alle, Zigarettenautomaten fast alle reibungslos funktionieren.\n",
      "Tagesgeld: Der Zinssatz ist nicht alles  Die Leitzinssenkung der EZB schlägt derzeit noch nicht auf Tagesgeld durch. Der passende Anbieter hängt aber von mehr als nur dem Zins ab. Ein Vergleich lohnt.\n",
      "Südafrika: Fassungslosigkeit über das milde Pistorius-UrteilPistorius schien sich zu verbeugen, Steenkamps Eltern blieben gefasst, ein Raunen ging durch den Saal: Das Urteil wegen fahrlässiger Tötung halten Südafrikas Frauen für einen schlechten Präzedenzfall.\n",
      "Islamischer Staat: Warum die Türkei bei der Anti-Terror-Allianz fehltErst unterstützte die Türkei indirekt islamische Extremisten in Syrien, nun will sie nicht in die von den USA geführte Koalition gegen den Islamischen Staat eintreten. Ankara hat eine eigene Agenda.\n",
      "Terrormiliz: Bundesregierung verbietet IS in DeutschlandInnenminister Thomas de Maizière hat das Verbot der Extremistengruppe IS in Deutschland verkündet, auch ihre Symbole dürfen nicht mehr zeigt werden. Er sieht die Sicherheit in Deutschland bedroht.\n",
      "Verkehrspolitik: Große Koalition sabotiert Maut-Debatte im BundestagAls Minister Dobrindt im Bundestag seine Pläne vorstellt, weichen Union und SPD allen Sachthemen aus. Dabei sind sie beim Thema Verkehr fundamental zerstritten: von der Pkw-Maut bis zum Straßenbau.\n",
      "Separatismus: Angst vor dem Zerfall Europas lässt Märkte bebenAm 18. September stimmen die Schotten über einen eigenen Staat ab. Auch Katalonien und Flandern streben nach Unabhängigkeit. Investoren fürchten den Separatismus. Doch nicht alle Effekte sind negativ.\n",
      "Sanktionen: Die Folterwerkzeuge der EU werden Moskau wehtunDie nächste Stufe der Sanktionen ist erreicht. Das Maßnahmenpaket gegen Moskau könnte reiche Russen unzufrieden machen und ihren Präsidenten Wladimir Putin in ernsthafte Schwierigkeiten bringen.\n",
      "Chinas Machtanspruch: Der nächste Weltkrieg könnte in Asien beginnenDas Pulverfass, das unser Weltgefüge sprengen kann, liegt in Asien mitten im Meer. Winzige Inselchen könnten einen Flächenbrand entfachen. Ein Angriff Chinas auf Japan ist mehr als düstere Fantasie.\n",
      "Umweltpolitik: Geht doch! Wie wir das Ozonloch repariertenVor 40 Jahren wurde erstmals vor dem Verlust der Schutzschicht gegen die UV-Strahlung gewarnt. Heute ist sie fast wieder intakt. Wie damals diskutiert wurde, erinnert an heutige Umweltdebatten.\n",
      "Apple: Neue iPhones jetzt schon ausverkauftWenige Stunden nach dem Vorbestellstart sind einige Modelle des iPhone 6 bereits ausverkauft. Drei bis vier Wochen müssen Kunden auf ihre Geräte warten. Telekom und Vodafone melden Bestellrekorde.\n",
      "Vor dem Referendum: Englands \"Schlächter\" brachte Schottland zur RäsonEin planloses Heldenstück führte 1746 zur endgültigen Unterwerfung Schottlands unter die englische Herrschaft. Der Aufstand des Stuart \"Bonnie Prince Charlie\" endete bei Culloden in einem Blutbad.\n",
      "Number of Lines:   48\n"
     ]
    }
   ],
   "source": [
    "filename=\"../Data/MultiNewsFeeds2014-09-12.txt\"\n",
    "#filename=\"../Data/MultiNewsFeeds2016-10-14.txt\"\n",
    "listOfNews=[]\n",
    "with open(filename,\"r\",encoding=\"utf-8\") as fin:\n",
    "    for line in fin:\n",
    "        line = line.strip()\n",
    "        print(line)\n",
    "        listOfNews.append(line)\n",
    "print(\"Number of Lines:  \",len(listOfNews))\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split documents into words, normalize and remove stopwords\n",
    "In _listOfNews_ each document is stored as a single string variable. Each of these document-strings is now split into a set of words. All words are transformed to a lower-case representation and stop-words are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwordlist=stopwords.words('german')\n",
    "docWords = [[word.strip('?!.:\",') for word in document.lower().split() \n",
    "             if word.strip('?!.:\",') not in stopwordlist] for document in listOfNews]\n",
    "#print(docWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the list of words of the first 5 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ document 0 ----------\n",
      "﻿kommunikation\n",
      "gut\n",
      "kommunizieren\n",
      "macht\n",
      "glücklich\n",
      "kaum\n",
      "psychologe\n",
      "friedemann\n",
      "schulz\n",
      "thun\n",
      "untersucht\n",
      "kommunikation\n",
      "ausmacht\n",
      "neuen\n",
      "buch\n",
      "entwickelt\n",
      "quintessenz\n",
      "davon\n",
      "------ document 1 ----------\n",
      "ukraine-krise\n",
      "bundesregierung\n",
      "toleriert\n",
      "ukrainischen\n",
      "mauerbau\n",
      "kanzleramt\n",
      "verständnis\n",
      "pläne\n",
      "ukrainischen\n",
      "regierung\n",
      "mauer\n",
      "entlang\n",
      "grenze\n",
      "russland\n",
      "bauen\n",
      "sei\n",
      "allein\n",
      "entscheidung\n",
      "ukraine\n",
      "------ document 2 ----------\n",
      "radfahren\n",
      "deutsche\n",
      "bringt\n",
      "chinesen\n",
      "aufs\n",
      "fixie\n",
      "einst\n",
      "peking\n",
      "fahrrad-welthauptstadt\n",
      "kamen\n",
      "autos\n",
      "ines\n",
      "brunn\n",
      "kämpft\n",
      "dagegen\n",
      "inhaberin\n",
      "fixie-shops\n",
      "chinesen\n",
      "aufs\n",
      "rad\n",
      "bringen\n",
      "------ document 3 ----------\n",
      "zurück\n",
      "wildnis\n",
      "\n",
      "jenseits\n",
      "komforts\n",
      "drei\n",
      "jahre\n",
      "lang\n",
      "suchte\n",
      "fotograf\n",
      "antoine\n",
      "bruy\n",
      "aussteiger\n",
      "europa\n",
      "bilder\n",
      "zeigen\n",
      "menschen\n",
      "konsum-\n",
      "leistungsgesellschaft\n",
      "rücken\n",
      "gekehrt\n",
      "------ document 4 ----------\n",
      "antisemitismus\n",
      "zentralrat\n",
      "beklagt\n",
      "schockwellen\n",
      "judenhass\n",
      "judenfeindliche\n",
      "hetze\n",
      "protesten\n",
      "israel\n",
      "juden\n",
      "tief\n",
      "getroffen\n",
      "sagte\n",
      "vorsitzende\n",
      "zentralrats\n",
      "kundgebung\n",
      "zeichen\n",
      "setzen\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "for doc in docWords[:5]:\n",
    "    print('------ document %d ----------'%idx)\n",
    "    for d in doc:\n",
    "        print(d)\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dictionary\n",
    "The elements of the list _docWords_ are itself lists. Each of these lists contains all relevant words of a document. The set of all relevant words in the document collection, i.e. relevant words, which appear in at least one document, are stored in a [gensim-dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html). In the dictionary to each of the relevant words an unique integer ID is assigned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c9a69b600a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multiNews.dict'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# store the dictionary, for future reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/books/lib/python3.8/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/books/lib/python3.8/site-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/books/lib/python3.8/site-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/books/lib/python3.8/site-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/books/lib/python3.8/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/books/lib/python3.8/site-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary(docWords)\n",
    "dictionary.save('multiNews.dict') # store the dictionary, for future reference\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents in the dictionary:  48\n",
      "Total number of corpus positions:  975\n",
      "Total number of non-zeros in the BoW-Matrix:  914\n",
      "Total number of different words in the dictionary:  811\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of documents in the dictionary: \",dictionary.num_docs)\n",
    "print(\"Total number of corpus positions: \",dictionary.num_pos)\n",
    "print(\"Total number of non-zeros in the BoW-Matrix: \",dictionary.num_nnz)\n",
    "print(\"Total number of different words in the dictionary: \",len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Word (BoW) representation\n",
    "Now arbitrary text-strings can be efficiently represented with respect to this dictionary. E.g. the code snippet below demonstrates how the text string _\"putin beschützt russen\"_ is represented as a list of tuples. The first element of such a tuple is the dictionary index of a word in the text-string and the second number defines how often this word occurs in the text-string. The list contains only tuples for words which occur in the text-string and in the dictionary. This representation is called **sparse Bag of Word** representation (sparse because it contains only the non-zero elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse BoW representation of putin beschützt russen: [(189, 1), (397, 1), (729, 1)]\n",
      "Index 189 refers to word beschützt. Frequency of this word in the document is 1\n",
      "Index 397 refers to word putin. Frequency of this word in the document is 1\n",
      "Index 729 refers to word russen. Frequency of this word in the document is 1\n"
     ]
    }
   ],
   "source": [
    "newDoc = \"putin beschützt russen\"\n",
    "newVec = dictionary.doc2bow(newDoc.lower().split())\n",
    "print(\"Sparse BoW representation of %s: %s\"%(newDoc,newVec))\n",
    "for idx,freq in newVec:\n",
    "    print(\"Index %d refers to word %s. Frequency of this word in the document is %d\"%(idx,dictionary[idx],freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this output we infer, that \n",
    "\n",
    "* the word at index 189 in the dictionary, which is _beschützt_ , appears once in the text-string _newDoc_\n",
    "* the word at index 397 in the dictionary, which is _putin_ , appears once in the text-string _newDoc_\n",
    "* the word at index 729 in the dictionary, which is _russen_ , appears once in the text-string _newDoc_ .\n",
    "\n",
    "The text-string _\"schottland stimmt ab\"_ is represented as a list of 2 tuples (see code snippet below). The first says, that the word at index 229 ( _ab_ ) appears once, the second tuple says that the word at index _807_ ( _schottland_ ) also appears once in the text-string. Since the word _stimmt_ does not appear in the dictionary, there is no corresponding tuple for this word in the list.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse BoW representation of schottland stimmt ab: [(229, 1), (807, 1)]\n",
      "Index 229 refers to word ab. Frequency of this word in the document is 1\n",
      "Index 807 refers to word schottland. Frequency of this word in the document is 1\n"
     ]
    }
   ],
   "source": [
    "newDoc2 = \"schottland stimmt ab\"\n",
    "newVec2 = dictionary.doc2bow(newDoc2.lower().split())\n",
    "print(\"Sparse BoW representation of %s: %s\"%(newDoc2,newVec2))\n",
    "for idx,freq in newVec2:\n",
    "    print(\"Index %d refers to word %s. Frequency of this word in the document is %d\"%(idx,dictionary[idx],freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Corpus Representation\n",
    "A corpus is a collection of documents. Such corpora may be annotated with meta-information, e.g. each word is tagged with its part of speech (POS-Tag). In this notebook, the list _docWords_, is a corpus without any annotations. So far this corpus has been applied to build the dictionary. In practical NLP tasks corpora are usually very large and therefore require an efficient representation. Using the already generated dictionary, each document (list of relevant words in a document) in the list _docWords_ can be transformed to its sparse BoW representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- First 10 documents of the corpus ---------------------------------\n",
      "-------------document 0 ---------------\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "-------------document 1 ---------------\n",
      "[(18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1)]\n",
      "-------------document 2 ---------------\n",
      "[(36, 2), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1)]\n",
      "-------------document 3 ---------------\n",
      "[(55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1)]\n",
      "-------------document 4 ---------------\n",
      "[(76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1)]\n",
      "-------------document 5 ---------------\n",
      "[(94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1)]\n",
      "-------------document 6 ---------------\n",
      "[(42, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 2), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1)]\n",
      "-------------document 7 ---------------\n",
      "[(125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 2), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1)]\n",
      "-------------document 8 ---------------\n",
      "[(140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 2), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1)]\n",
      "-------------document 9 ---------------\n",
      "[(55, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docWords]\n",
    "corpora.MmCorpus.serialize('multiNews.mm', corpus)\n",
    "print(\"------------------------- First 10 documents of the corpus ---------------------------------\")\n",
    "idx=0\n",
    "for d in corpus[0:10]:\n",
    "    print(\"-------------document %d ---------------\" %idx)\n",
    "    print(d)\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Analysis\n",
    "Typical information retrieval tasks include the task of determining similarities between collections of documents or between a query and a collection of documents. Using gensim a fast similarity calculation and search is supported. For this, first a **cosine-similarity-index** of the given corpus is calculated as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(corpus, num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assume that for a given query, e.g. _\"putin beschützt russen\"_ the best matching document in the corpus must be determined. The sparse BoW representation of this query has already been calculated and stored in the variable _newVec_. The similarity between this query and all documents in the corpus can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.13608277), (12, 0.0), (13, 0.0), (14, 0.0), (15, 0.0), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.12309149), (24, 0.0), (25, 0.0), (26, 0.0), (27, 0.0), (28, 0.0), (29, 0.0), (30, 0.0), (31, 0.0), (32, 0.0), (33, 0.0), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.0), (41, 0.0), (42, 0.0), (43, 0.23570226), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[newVec]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuples in this output contain as first element the index of the document in the corpus. The second element is the cosine-similarity between this corpus-document and the query. \n",
    "\n",
    "In order to get a sorted list of increasingly similar documents, the `argsort()`-method can be applied as shown below. The last value in this list is the index of the most similar document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 25 26 27 28 29 30 31 32 33 24 34 36 37 38 39 40 41 42 44 45 35 46 47\n",
      " 21  1  2  3  4  5  6  7  8  9 22 10 12 13 14 15 16 17 18 19 20 23 11 43]\n"
     ]
    }
   ],
   "source": [
    "print(sims.argsort())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example _document 43_ best matches to the query. The cosine-similarity between the query and _document 43_ is _0.2357_.\n",
    "\n",
    "Question: Manually verify the calculated similiarity value between the query and _document 43_.\n",
    "\n",
    "In the same way the similarity between documents in the corpus can be calculated. E.g. the similiarity between _document 1_ and all other documents in the corpus is determined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 1.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0), (11, 0.0), (12, 0.0), (13, 0.05143445), (14, 0.0), (15, 0.15877683), (16, 0.0), (17, 0.0), (18, 0.0), (19, 0.0), (20, 0.0), (21, 0.0), (22, 0.0), (23, 0.0), (24, 0.0), (25, 0.0), (26, 0.044543542), (27, 0.0727393), (28, 0.0), (29, 0.0), (30, 0.05006262), (31, 0.048795003), (32, 0.0), (33, 0.04761905), (34, 0.0), (35, 0.0), (36, 0.0), (37, 0.0), (38, 0.0), (39, 0.0), (40, 0.04364358), (41, 0.045501575), (42, 0.0), (43, 0.0), (44, 0.0), (45, 0.0), (46, 0.0), (47, 0.0)]\n",
      "[ 0 22 46 24 25 28 29 32 34 35 36 37 38 39 42 43 44 45 21 20 23 18  2  3\n",
      "  4  5  6  7  8  9 19 11 10 12 14 16 17 47 40 26 41 33 31 30 13 27 15  1]\n"
     ]
    }
   ],
   "source": [
    "sims = index[corpus[1]]\n",
    "print((list(enumerate(sims))))\n",
    "print(sims.argsort())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus _document 15_ is the most similar document to _document 1_. As can easily be verified both documents refer to the same topic (crisis in ukraine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF representation\n",
    "So far in the BoW representation of the documents the _term frequency (tf)_ has been applied. This value measures how often the term (word) appears in the document. If document similarity is calculated on such tf-based BoW representation, common words which appear quite often (in many documents) but have low semantic focus have a strong impact on the similarity-value. In most cases this is a drawback, since similarity should be based on terms with a high semantic focus. Such semantically meaningful words usually appear only in a few documents. The _term frequency inversed document frequency measure (tf-idf)_ does not only count the frequency of a term in a document, but weighs those terms stronger, which occur only in a few documents of the corpus. \n",
    "\n",
    "In _gensim_ the _tfidf_ - model of a corpus can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _tf-idf_-representation of the first 3 documents in the corpus are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------tf-idf BoW of document 0 ---------------\n",
      "[(0, 0.24353425982511087), (1, 0.1999289070955868), (2, 0.24353425982511087), (3, 0.24353425982511087), (4, 0.24353425982511087), (5, 0.24353425982511087), (6, 0.24353425982511087), (7, 0.24353425982511087), (8, 0.24353425982511087), (9, 0.24353425982511087), (10, 0.1999289070955868), (11, 0.1744214109180963), (12, 0.24353425982511087), (13, 0.24353425982511087), (14, 0.24353425982511087), (15, 0.24353425982511087), (16, 0.24353425982511087), (17, 0.24353425982511087)]\n",
      "-------------tf-idf BoW of document 1 ---------------\n",
      "[(18, 0.23522128928414127), (19, 0.23522128928414127), (20, 0.16846758720673122), (21, 0.23522128928414127), (22, 0.23522128928414127), (23, 0.23522128928414127), (24, 0.23522128928414127), (25, 0.23522128928414127), (26, 0.23522128928414127), (27, 0.19310439248245848), (28, 0.19310439248245848), (29, 0.16846758720673122), (30, 0.16846758720673122), (31, 0.23522128928414127), (32, 0.19310439248245848), (33, 0.16846758720673122), (34, 0.47044257856828253), (35, 0.23522128928414127)]\n",
      "-------------tf-idf BoW of document 2 ---------------\n",
      "[(36, 0.4166329302664041), (37, 0.20831646513320204), (38, 0.17101693714061422), (39, 0.17101693714061422), (40, 0.20831646513320204), (41, 0.4166329302664041), (42, 0.17101693714061422), (43, 0.17101693714061422), (44, 0.17101693714061422), (45, 0.20831646513320204), (46, 0.20831646513320204), (47, 0.20831646513320204), (48, 0.20831646513320204), (49, 0.20831646513320204), (50, 0.20831646513320204), (51, 0.17101693714061422), (52, 0.20831646513320204), (53, 0.20831646513320204), (54, 0.20831646513320204)]\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "for d in corpus[:3]:\n",
    "    print(\"-------------tf-idf BoW of document %d ---------------\" %idx)\n",
    "    print(tfidf[d])\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this representation the second element in the tuples is not the term frequency, but the _tfidf_. Note that default configuration of [tf-idf in gensim](http://radimrehurek.com/gensim/models/tfidfmodel.html) calculates tf-idf values such that each document-vector has a norm of _1._ The tfidf-model without normalization is generated at the end of this notebook.\n",
    "\n",
    "Question: Find the maximum tf-idf value in these 3 documents. To which word does this maximum value belong? How often does this word occur in the document?\n",
    "\n",
    "The _tf-idf_-representation of the text-string _\"putin beschützt russen\"_ is determined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf BoW representation of putin beschützt russen is:\n",
      " [(189, 1), (397, 1), (729, 1)]\n",
      "tf-idf BoW representation of putin beschützt russen is:\n",
      " [(189, 0.6115372747558391), (397, 0.5020401609118563), (729, 0.6115372747558391)]\n"
     ]
    }
   ],
   "source": [
    "newVecTfIdf = tfidf[newVec]\n",
    "print(\"tf BoW representation of %s is:\\n %s\"%(newDoc,newVec))\n",
    "print(\"tf-idf BoW representation of %s is:\\n %s\"%(newDoc,newVecTfIdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Explain the different values in the tfidf BoW representation _newVecTfIdf_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF-Model without normalization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfnoNorm = models.TfidfModel(corpus,normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display tf-idf BoW of first 3 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------tf-idf BoW of document 0 ---------------\n",
      "[(0, 5.584962500721157), (1, 4.584962500721157), (2, 5.584962500721157), (3, 5.584962500721157), (4, 5.584962500721157), (5, 5.584962500721157), (6, 5.584962500721157), (7, 5.584962500721157), (8, 5.584962500721157), (9, 5.584962500721157), (10, 4.584962500721157), (11, 4.0), (12, 5.584962500721157), (13, 5.584962500721157), (14, 5.584962500721157), (15, 5.584962500721157), (16, 5.584962500721157), (17, 5.584962500721157)]\n",
      "-------------tf-idf BoW of document 1 ---------------\n",
      "[(18, 5.584962500721157), (19, 5.584962500721157), (20, 4.0), (21, 5.584962500721157), (22, 5.584962500721157), (23, 5.584962500721157), (24, 5.584962500721157), (25, 5.584962500721157), (26, 5.584962500721157), (27, 4.584962500721157), (28, 4.584962500721157), (29, 4.0), (30, 4.0), (31, 5.584962500721157), (32, 4.584962500721157), (33, 4.0), (34, 11.169925001442314), (35, 5.584962500721157)]\n",
      "-------------tf-idf BoW of document 2 ---------------\n",
      "[(36, 11.169925001442314), (37, 5.584962500721157), (38, 4.584962500721157), (39, 4.584962500721157), (40, 5.584962500721157), (41, 11.169925001442314), (42, 4.584962500721157), (43, 4.584962500721157), (44, 4.584962500721157), (45, 5.584962500721157), (46, 5.584962500721157), (47, 5.584962500721157), (48, 5.584962500721157), (49, 5.584962500721157), (50, 5.584962500721157), (51, 4.584962500721157), (52, 5.584962500721157), (53, 5.584962500721157), (54, 5.584962500721157)]\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "for d in corpus[:3]:\n",
    "    print(\"-------------tf-idf BoW of document %d ---------------\" %idx)\n",
    "    print(tfidfnoNorm[d])\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the tf-idf-values as calculated in the code-cell above, by own tf-idf-formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.584962500721156\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf=1 #term frequency\n",
    "NumDocs=dictionary.num_docs #number of documents\n",
    "df=1 #number of documents in which the word appears\n",
    "tfidf=tf*np.log2(float(NumDocs)/df)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation and Document models with Keras\n",
    "This section demonstrates how [Keras](https://keras.io/api/preprocessing/text/) can be applied for tokenisation and BoW document-modelling. I.e. no new techniques are introduced here. Instead it is shown how Keras can be applied to implement already known procedures. This is usefunl, because Keras will be applied later on to implement Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizsation\n",
    "\n",
    "#### Text collections as lists of strings\n",
    "Tokens are atomic text elements. Depending on the NLP task and the selected approach to solve this task, tokens can either be\n",
    "* characters\n",
    "* words (uni-grams)\n",
    "* n-grams\n",
    "\n",
    "Single texts are often represented as variables of type `string`. Collections of texts are then represented as lists of strings.\n",
    "\n",
    "Below, a collection of 3 texts is generated as a list of `string`-variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"\"\"Florida shooting: Nikolas Cruz confesses to police Nikolas Cruz is said\n",
    "to have killed 17 people before escaping and visiting a McDonalds.\"\"\"\n",
    "text2=\"\"\"Winter Olympics: Great Britain's Dom Parsons wins skeleton bronze medal\n",
    "Dom Parsons claims Great Britain's first medal of 2018 Winter Olympics with bronze in the men's skeleton.\"\"\"\n",
    "text3=\"\"\"Theresa May to hold talks with Angela Merkel in Berlin\n",
    "The prime minister's visit comes amid calls for the UK to say what it wants from Brexit.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida shooting: Nikolas Cruz confesses to police Nikolas Cruz is said\n",
      "to have killed 17 people before escaping and visiting a McDonalds.\n"
     ]
    }
   ],
   "source": [
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist=[text1,text2,text3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras class Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras methods for preprocessing texts are contained in `keras.preprocessing.text`. From this module, we apply the `Tokenizer`-class to \n",
    "* transform words to integers, i.e. generating a word-index\n",
    "* represent texts as sequences of integers\n",
    "* represent collections of texts in a Bag-of-Words (BOW)-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a `Tokenizer`-object and fit it on the given list of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=text.Tokenizer()\n",
    "tokenizer.fit_on_texts(textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Tokenizer`-class accepts a list of arguments, which can be configured at initialisation of a `Tokenizer`-object. The default-values are printed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured maximum number of words in the vocabulary:  None\n",
      "Configured filters:  !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
      "\n",
      "Map all characters to lower case:  True\n",
      "Tokenizsation on character level:  False\n"
     ]
    }
   ],
   "source": [
    "print(\"Configured maximum number of words in the vocabulary: \",tokenizer.num_words) #Maximum number of words to regard in the vocabulary\n",
    "print(\"Configured filters: \",tokenizer.filters) #characters to ignore in tokenization\n",
    "print(\"Map all characters to lower case: \",tokenizer.lower) #Mapping of characters to lower-case\n",
    "print(\"Tokenizsation on character level: \",tokenizer.char_level) #whether tokens are words or characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents: \",tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar as the `dictionary` in gensim (see above), the Keras `Tokenizer` provides a word-index, which uniquely maps each word to an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of words:  {'to': 1, 'the': 2, 'nikolas': 3, 'cruz': 4, 'winter': 5, 'olympics': 6, 'great': 7, \"britain's\": 8, 'dom': 9, 'parsons': 10, 'skeleton': 11, 'bronze': 12, 'medal': 13, 'with': 14, 'in': 15, 'florida': 16, 'shooting': 17, 'confesses': 18, 'police': 19, 'is': 20, 'said': 21, 'have': 22, 'killed': 23, '17': 24, 'people': 25, 'before': 26, 'escaping': 27, 'and': 28, 'visiting': 29, 'a': 30, 'mcdonalds': 31, 'wins': 32, 'claims': 33, 'first': 34, 'of': 35, '2018': 36, \"men's\": 37, 'theresa': 38, 'may': 39, 'hold': 40, 'talks': 41, 'angela': 42, 'merkel': 43, 'berlin': 44, 'prime': 45, \"minister's\": 46, 'visit': 47, 'comes': 48, 'amid': 49, 'calls': 50, 'for': 51, 'uk': 52, 'say': 53, 'what': 54, 'it': 55, 'wants': 56, 'from': 57, 'brexit': 58}\n"
     ]
    }
   ],
   "source": [
    "print(\"Index of words: \",tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `word_docs()` returns for each word the number of documents, in which the word appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs, in which word appears:  defaultdict(<class 'int'>, {'a': 1, 'people': 1, 'cruz': 1, 'killed': 1, 'florida': 1, 'escaping': 1, 'mcdonalds': 1, 'to': 2, 'shooting': 1, 'and': 1, 'police': 1, 'have': 1, 'confesses': 1, 'nikolas': 1, 'before': 1, 'visiting': 1, 'is': 1, '17': 1, 'said': 1, 'the': 2, 'winter': 1, 'olympics': 1, 'dom': 1, 'medal': 1, 'of': 1, \"britain's\": 1, 'parsons': 1, 'first': 1, 'with': 2, \"men's\": 1, 'in': 2, 'wins': 1, 'skeleton': 1, 'great': 1, '2018': 1, 'bronze': 1, 'claims': 1, 'wants': 1, 'theresa': 1, 'uk': 1, 'it': 1, 'hold': 1, 'brexit': 1, 'merkel': 1, \"minister's\": 1, 'calls': 1, 'prime': 1, 'comes': 1, 'berlin': 1, 'talks': 1, 'what': 1, 'say': 1, 'for': 1, 'visit': 1, 'angela': 1, 'amid': 1, 'may': 1, 'from': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of docs, in which word appears: \",tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent texts as sequences of word-indices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following representation of texts as sequences of word-indicees is a common input to Neural Networks implemented in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 0 sequence:  [16, 17, 3, 4, 18, 1, 19, 3, 4, 20, 21, 1, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "text 1 sequence:  [5, 6, 7, 8, 9, 10, 32, 11, 12, 13, 9, 10, 33, 7, 8, 34, 13, 35, 36, 5, 6, 14, 12, 15, 2, 37, 11]\n",
      "text 2 sequence:  [38, 39, 1, 40, 41, 14, 42, 43, 15, 44, 2, 45, 46, 47, 48, 49, 50, 51, 2, 52, 1, 53, 54, 55, 56, 57, 58]\n"
     ]
    }
   ],
   "source": [
    "textSeqs=tokenizer.texts_to_sequences(textlist)\n",
    "for i,ts in enumerate(textSeqs):\n",
    "    print(\"text %d sequence: \"%i,ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent text-collection as binary BoW:\n",
    "A Bag-Of-Words representation of documents contains $N$ rows and $|V|$ columns, where $N$ is the number of documents in the collection and $|V|$ is the size of the vocabulary, i.e. the number of different words in the entire document collection.\n",
    "\n",
    "The entry $x_{i,j}$ of the BoW-Matrix indicates the **relevance of word $j$ in document $i$**.\n",
    "\n",
    "In this lecture 3 different types of **word-relevance** are considered:\n",
    "\n",
    "1. **Binary BoW:** Entry $x_{i,j}$ is *1* if word $j$ appears in document $i$, otherwise 0.\n",
    "2. **Count-based BoW:** Entry $x_{i,j}$ is the frequency of word $j$ in document $i$.\n",
    "3. **Tf-idf-based BoW:** Entry $x_{i,j}$ is the tf-idf of word $j$ with respect to document $i$.\n",
    "\n",
    "The BoW-representation of texts is a common input to conventional Machine Learning algorithms (not Neural Netorks like CNN and RNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_matrix(textlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count-based BoW\n",
    "Represent text-collection as BoW with word-counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_matrix(textlist,mode=\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf-based BoW\n",
    "In the BoW representation above the term frequency (tf) has been applied. This value measures how often the term (word) appears in the document. If document similarity is calculated on such tf-based BoW representation, common words which appear quite often (in many documents) but have low semantic focus, have a strong impact on the similarity-value. In most cases this is a drawback, since similarity should be based on terms with a high semantic focus. Such semantically meaningful words usually appear only in a few documents. The term frequency inversed document frequency measure (tf-idf) does not only count the frequency of a term in a document, but weighs those terms stronger, which occur only in a few documents of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.17360019 0.         1.55141507 1.55141507 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.91629073 0.91629073\n",
      "  0.91629073 0.91629073 0.91629073 0.91629073 0.91629073 0.91629073\n",
      "  0.91629073 0.91629073 0.91629073 0.91629073 0.91629073 0.91629073\n",
      "  0.91629073 0.91629073 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.69314718 0.         0.         1.55141507\n",
      "  1.55141507 1.55141507 1.55141507 1.55141507 1.55141507 1.55141507\n",
      "  1.55141507 1.55141507 0.69314718 0.69314718 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.91629073 0.91629073 0.91629073 0.91629073\n",
      "  0.91629073 0.91629073 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         1.17360019 1.17360019 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.69314718 0.69314718 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.91629073 0.91629073 0.91629073 0.91629073\n",
      "  0.91629073 0.91629073 0.91629073 0.91629073 0.91629073 0.91629073\n",
      "  0.91629073 0.91629073 0.91629073 0.91629073 0.91629073 0.91629073\n",
      "  0.91629073 0.91629073 0.91629073 0.91629073 0.91629073]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_matrix(textlist,mode=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}