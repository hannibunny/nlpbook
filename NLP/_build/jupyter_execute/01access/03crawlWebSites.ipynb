{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download HTML Files\n",
    "\n",
    "- Author:      Johannes Maucher\n",
    "- Last update: 2018-10-12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T20:08:46.410000Z",
     "start_time": "2017-11-05T21:08:43.849000+01:00"
    }
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from slugify import slugify\n",
    "import datetime\n",
    "import os\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define lists of news domains for different languages and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-22\n"
     ]
    }
   ],
   "source": [
    "today=datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T20:08:51.547000Z",
     "start_time": "2017-11-05T21:08:51.530000+01:00"
    }
   },
   "outputs": [],
   "source": [
    "cat=\"GENERAL\"\n",
    "#cat=\"TECH\"\n",
    "#cat=\"SPORT\"\n",
    "lang=\"GERMAN\"\n",
    "#lang=\"ENGLISH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T20:08:53.702000Z",
     "start_time": "2017-11-05T21:08:53.671000+01:00"
    }
   },
   "outputs": [],
   "source": [
    "general_sources_de = ['http://www.zeit.de',\n",
    "                      'http://www.spiegel.de/',\n",
    "                      'http://www.welt.de/',\n",
    "                      'http://www.sueddeutsche.de',\n",
    "                      'http://www.faz.net'\n",
    "                     ]\n",
    "\n",
    "general_sources_en = ['https://www.washingtonpost.com',\n",
    "                      'http://www.nytimes.com/',\n",
    "                      'http://www.chicagotribune.com/',\n",
    "                      'http://www.bostonherald.com/',\n",
    "                      'http://www.sfchronicle.com/']\n",
    "\n",
    "tech_sources_de=['http://chip.de/',\n",
    "                 'http://t-online.de',\n",
    "                 'http://www.computerbild.de',\n",
    "                 'http://www.heise.de',\n",
    "                 'http://www.golem.de']\n",
    "\n",
    "tech_sources_en=['http://radar.oreilly.com/',\n",
    "                 'https://www.cnet.com/news/',\n",
    "                 'http://www.techradar.com/news/computing'\n",
    "                ]                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T20:08:55.515000Z",
     "start_time": "2017-11-05T21:08:55.483000+01:00"
    }
   },
   "outputs": [],
   "source": [
    "if lang==\"ENGLISH\":\n",
    "    if cat==\"GENERAL\":\n",
    "        sources=general_sources_en\n",
    "    else:\n",
    "        sources=tech_sources_en\n",
    "else:\n",
    "    if cat==\"GENERAL\":\n",
    "        sources=general_sources_de\n",
    "    else:\n",
    "        sources=tech_sources_de\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download subdomain HTML pages \n",
    "The below defined function `crawl()` determines all subdomains of the specified url and saves the HTML files of these subdomains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T20:08:58.703000Z",
     "start_time": "2017-11-05T21:08:58.593000+01:00"
    }
   },
   "outputs": [],
   "source": [
    "def crawl(url,maxSubSites=5,category=\"GENERAL\",lang=\"GERMAN\"):\n",
    "    domain = url.split(\"//www.\")[-1].split(\"/\")[0]\n",
    "    print(domain)\n",
    "    dirname=lang+\"/\"+category+\"/\"+\"HTML/\"+domain.split('.')[0]+\"-\"+today\n",
    "    try:\n",
    "        os.makedirs(dirname)\n",
    "    except:\n",
    "        print(\"Directory %s already exists.\"%dirname)\n",
    "    filename = dirname+\"/\"+domain.split('.')[0] + '.html'\n",
    "    html = requests.get(url).content\n",
    "    with open(filename, 'wb') as f:\n",
    "        #print filename\n",
    "        f.write(html)\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    links = set(soup.find_all('a', href=True)) #find all links in this page\n",
    "    count=0\n",
    "    for link in links:\n",
    "        if count > maxSubSites:\n",
    "            break\n",
    "        sub_url = link['href']\n",
    "        page_name = link.string\n",
    "        if domain in sub_url:\n",
    "            count+=1\n",
    "            try:\n",
    "                page = requests.get(sub_url).content\n",
    "                filename = dirname+\"/\"+slugify(page_name).lower() + '.html'\n",
    "                with open(filename, 'wb') as f:\n",
    "                    #print filename\n",
    "                    f.write(page)\n",
    "            except:\n",
    "                pass\n",
    "    return dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T20:13:35.332000Z",
     "start_time": "2017-11-05T21:08:59.968000+01:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeit.de\n"
     ]
    }
   ],
   "source": [
    "htmlDirs=[]\n",
    "for url in sources:\n",
    "    htmldir=crawl(url,maxSubSites=50,category=cat,lang=lang)\n",
    "    htmlDirs.append(htmldir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T20:13:35.356000Z",
     "start_time": "2017-11-05T21:13:35.346000+01:00"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GERMAN/GENERAL/HTML/zeit-2020-09-09', 'GERMAN/GENERAL/HTML/spiegel-2020-09-09', 'GERMAN/GENERAL/HTML/welt-2020-09-09', 'GERMAN/GENERAL/HTML/sueddeutsche-2020-09-09', 'GERMAN/GENERAL/HTML/faz-2020-09-09']\n"
     ]
    }
   ],
   "source": [
    "print(htmlDirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl raw text from local HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T13:41:04.312000Z",
     "start_time": "2017-10-20T13:41:04.303000Z"
    }
   },
   "outputs": [],
   "source": [
    "TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p']\n",
    "def read_html(path,minlen=15):\n",
    "    with open(path, 'r') as f:\n",
    "        html = f.read()\n",
    "        soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "        for tag in soup.find_all(TAGS):\n",
    "            text = tag.get_text()\n",
    "            if len(text)>minlen:\n",
    "                yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-20T13:42:00.926000Z",
     "start_time": "2017-10-20T13:41:25.083000Z"
    }
   },
   "outputs": [],
   "source": [
    "rawtextlist=[]\n",
    "for dir in htmlDirs:\n",
    "    textdir=dir.replace(\"HTML\",\"TEXT\")\n",
    "    try:\n",
    "        os.makedirs(textdir)\n",
    "    except:\n",
    "        pass\n",
    "    for htmlfile in os.listdir(dir):\n",
    "        htmlpath=dir+\"/\"+htmlfile\n",
    "        rawText=read_html(htmlpath)\n",
    "        rawtextlist.append(rawText)\n",
    "        textpath=htmlpath.replace(\"HTML\",\"TEXT\")\n",
    "        textfilename=textpath.replace(\".html\",\".txt\")\n",
    "        with open(textfilename,\"w\") as f:\n",
    "            for cont in rawText:\n",
    "                f.write(cont)\n",
    "        f.close()\n",
    "#print len(rawtextlist)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Questions\n",
    "1. Inspect some of the created raw-text files and suggest improvements."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}