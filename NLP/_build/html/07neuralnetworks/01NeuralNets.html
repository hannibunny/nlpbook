
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10.1. Neural Networks Introduction &#8212; Natural Language Processing Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10.2. Recurrent Neural Networks" href="02RecurrentNeuralNetworks.html" />
    <link rel="prev" title="10. Neural Networks" href="07neuralnetworks.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Natural Language Processing Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01access/01access.html">
   1. Access and Preprocess Text
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/01AccessTextFromFile.html">
     1.1. Access and Analyse Contents of Textfiles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/02crawlHTMLcontent.html">
     1.2. Access Contents of HTML Page
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/03crawlWebSites.html">
     1.3. Download HTML Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/04crawlRSSFeeds.html">
     1.4. Access RSS Feed
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/05RegularExpressions.html">
     1.5. Regular expressions in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/06accessTweets.html">
     1.6. Access Tweets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02normalisation/02normalisation.html">
   2. Word Normalisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/01morphology.html">
     2.1. Morphology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/03StemLemma.html">
     2.2. TextBlob Stemming and Lemmatization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/01LevenstheinDistance.html">
     2.3. Correction of Spelling Errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03postagging/03postagging.html">
   3. Part-Of-Speech Tagging
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03postagging/01tagsetsAndAlgorithms.html">
     3.1. PoS Tagsets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03postagging/02PosTagging.html">
     3.3. POS Tagging with NLTK
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04ngram/04ngram.html">
   4. N-Gram Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05representations/05representations.html">
   5. Vector Representations of Words and Documents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05representations/01WordEmbeddingImplementation.html">
   6. Applying Word-Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05representations/02gensimDocModelSimple.html">
   7. Document models and similarity
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05representations/05topicextraction.html">
   8. Topic Extraction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05representations/05lsi.html">
     8.1. Latent Semantic Indexing (LSI)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05representations/02LatentSemanticIndexing.html">
     8.2. Implementation of Topic Extraction and Document Clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06classification/06classification.html">
   9. Text Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/06classificationMetrics.html">
     9.1. Validation of Classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/07classificationNaiveBayes.html">
     9.2. Naive Bayes Text Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/FakeNewsClassification.html">
     9.3. Text Classification Application: Fake News detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="07neuralnetworks.html">
   10. Neural Networks
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     10.1. Neural Networks Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02RecurrentNeuralNetworks.html">
     10.2. Recurrent Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03ConvolutionNeuralNetworks.html">
     10.3. Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04CNN.html">
     10.4. CNN, LSTM and Attention for IMDB Movie Review classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   11. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/07neuralnetworks/01NeuralNets.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/07neuralnetworks/01NeuralNets.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#natural-neuron">
   10.1.1. Natural Neuron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neuron">
   10.1.2. Artificial Neuron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-function">
     10.1.2.1. Activation Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias">
     10.1.2.2. Bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neural-networks-general-notions">
   10.1.3. Artificial Neural Networks: General Notions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layers">
     10.1.3.1. Layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feedforward-and-recurrent-neural-networks">
     10.1.3.2. Feedforward- and Recurrent Neural Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-concept-of-supervised-learning">
     10.1.3.3. General Concept of Supervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-and-stochastic-gradient-descent-learning">
     10.1.3.4. Gradient Descent and Stochastic Gradient Descent Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-learning">
     10.1.3.5. Gradient Descent Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#single-layer-perceptron">
   10.1.4. Single Layer Perceptron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-regression">
     10.1.4.1. SLP for Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-binary-classification">
     10.1.4.2. SLP for binary classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-classification-in-k-2-classes">
     10.1.4.3. SLP for classification in
     <span class="math notranslate nohighlight">
      \(K&gt;2\)
     </span>
     classes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-single-layer-perceptron">
   10.1.5. Summary Single Layer Perceptron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-layer-perceptron">
   10.1.6. Multi Layer Perceptron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notations-and-basic-characteristics">
     10.1.6.1. Notations and Basic Characteristics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#early-mlp-example-autonomos-driving">
   10.1.7. Early MLP Example: Autonomos Driving
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-networks-introduction">
<h1><span class="section-number">10.1. </span>Neural Networks Introduction<a class="headerlink" href="#neural-networks-introduction" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Author: Johannes Maucher</p></li>
<li><p>Last Update: 20.10.2020</p></li>
</ul>
<div class="section" id="natural-neuron">
<h2><span class="section-number">10.1.1. </span>Natural Neuron<a class="headerlink" href="#natural-neuron" title="Permalink to this headline">¶</a></h2>
<p><img alt="Natural Neuron" src="https://maucher.home.hdm-stuttgart.de/Pics/neuron.png" /></p>
<p>Neurons are the basic elements for information processing. A neuron consists of a cell-body, many dendrites and an axon. The neuron receives electrical signals from other neurons via the dendrites. In the cell-body all input-signals received via the dendrites are accumulated. If the accumulated electrical signal exceeds a certain threshold, the cell-body outputs an electrical signal via it’s axon. In this case the neuron is said to be activated. Otherwise, if the accumulated input at the cell-body is below the threshold, the neuron is not active, i.e. it does not send a signal to connected neurons. The point, where dendrites of neurons are connected to axons of other neurons is called synapse. The synapse consists of an electrochemical substance. The conductivity of this substance depends on it’s concentration of neurotransmitters. The process of learning adapts the conductivity of synapses and, i.e. the degree of connection between neurons. A single neuron can receive inputs from 10-100000 other neurons. However, there is only one axon, but multiple dendrites of other cell can be connected to this axon.</p>
</div>
<div class="section" id="artificial-neuron">
<h2><span class="section-number">10.1.2. </span>Artificial Neuron<a class="headerlink" href="#artificial-neuron" title="Permalink to this headline">¶</a></h2>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/slpPresentationSingle.png" width="400">
<p>The artificial model of a neuron is shown in the picture below. At the input of each neuron the weighted sum</p>
<div class="math notranslate nohighlight">
\[in=\sum\limits_{j=0}^d w_jx_j = \mathbf{w}\cdot \mathbf{x^T}=(w_0, w_1, \ldots, w_d) \cdot (x_0, x_1, \ldots, x_d)^T \]</div>
<p>is calculated. The values <span class="math notranslate nohighlight">\(x_j\)</span> are the outputs of other neurons. Each <span class="math notranslate nohighlight">\(x_j\)</span> is weighted by a scalar <span class="math notranslate nohighlight">\(w_j\)</span>, similar as in the natural model the signal-strength from a connected neuron is damped by the conductivity of the synapse. As in the natural model, learning of an artificial network means adaptation of the weights between neurons. Also, as in the natural model, the weighted sum at the input of the neuron is fed to an <strong>activation function g()</strong>, which can be a simple threshold-function that outputs a <code class="docutils literal notranslate"><span class="pre">1</span></code> if the weighted sum <span class="math notranslate nohighlight">\(in=\sum\limits_{j=0}^d w_jx_j\)</span>  exceeds a certain threshold and a <code class="docutils literal notranslate"><span class="pre">0</span></code> otherwise.</p>
<div class="section" id="activation-function">
<h3><span class="section-number">10.1.2.1. </span>Activation Function<a class="headerlink" href="#activation-function" title="Permalink to this headline">¶</a></h3>
<p>The most common activation functions are:</p>
<ul>
<li><p><strong>Threshold:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}g(in)= \left\lbrace \begin{array}{ll} 1, &amp; in \geq 0 \\ 0, &amp; else \\ \end{array} \right.\end{split}\]</div>
</li>
<li><p><strong>Sigmoid:</strong></p>
<div class="math notranslate nohighlight">
\[g(in)=\frac{1}{1+exp(-in)}\]</div>
</li>
<li><p><strong>Tanh:</strong></p>
<div class="math notranslate nohighlight">
\[g(in)=\tanh(in)\]</div>
</li>
<li><p><strong>Identity:</strong></p>
<div class="math notranslate nohighlight">
\[
	g(in)=in
	\]</div>
</li>
<li><p><strong>ReLu:</strong></p>
<div class="math notranslate nohighlight">
\[g(in)=max\left( 0 , in \right)\]</div>
</li>
<li><p><strong>Softmax:</strong></p>
<div class="math notranslate nohighlight">
\[g(in_i,in_j)=\frac{\exp(in_i)}{\sum\limits_{j=1}^{K} \exp(in_j)}\]</div>
</li>
</ul>
<p><img alt="Activationfunctions" src="https://maucher.home.hdm-stuttgart.de/Pics/activationsViz.png" /></p>
<p>All artificial neurons calculate the sum of weighted inputs <span class="math notranslate nohighlight">\(in\)</span>. Neurons differ in the activation function, which is applied on <span class="math notranslate nohighlight">\(in\)</span>. In the sections below it will be described how to choose an appropriate activation function.</p>
</div>
<div class="section" id="bias">
<h3><span class="section-number">10.1.2.2. </span>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">¶</a></h3>
<p>Among the input-signals, <span class="math notranslate nohighlight">\(x_0\)</span> has a special meaning. In contrast to all other <span class="math notranslate nohighlight">\(x_j\)</span> the value of this so called <strong>bias</strong> is constant <span class="math notranslate nohighlight">\(x_0=1\)</span>. Instead of denoting the bias input to a neuron by <span class="math notranslate nohighlight">\(w_0 \cdot x_0 = w_0\)</span> it can also be written as <span class="math notranslate nohighlight">\(b\)</span>. I.e.</p>
<div class="math notranslate nohighlight">
\[in=\sum\limits_{j=0}^d w_jx_j  \quad \mbox{  is equivalent to  } \quad in=\sum\limits_{j=1}^d w_jx_j+b\]</div>
<p>Hence the following two graphical representations are equivalent:</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpPresentations.png" style="width: 700px;"/></div>
</div>
<div class="section" id="artificial-neural-networks-general-notions">
<h2><span class="section-number">10.1.3. </span>Artificial Neural Networks: General Notions<a class="headerlink" href="#artificial-neural-networks-general-notions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="layers">
<h3><span class="section-number">10.1.3.1. </span>Layers<a class="headerlink" href="#layers" title="Permalink to this headline">¶</a></h3>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/mlpL3.png" style="width: 600px;"/><p>In Neural Networks neurons are arranged in layers. All Neurons of a single layer are of the same type, i.e. they apply the same activation function on the weighted sum at their input (see previous section). Each Neural Network has at least one input-layer and one output-layer. The number of neurons in the input-layer is determined by the number of features (attributes) in the given Machine-Learning problem. The number of neurons in the output-layer depends on the task. E.g. for <strong>binary-classification</strong> and <strong>regression</strong> only one neuron in the output-layer is requried, for classification into <span class="math notranslate nohighlight">\(K&gt;2\)</span> classes the output-layer consists of <span class="math notranslate nohighlight">\(K\)</span> neurons.</p>
<p>Actually, the <strong>input-layer</strong> is not considered as a <em>real</em> layer, since it only takes in the values of the current feature-vector, but does not perform any processing, such as calculating an activation function of a weighted sum. The input layer is ignored when determining the number of layers in a neural-network.</p>
<p>For <strong>example</strong> for a binary credit-worthiness classification of customers, which are modelled by the numeric features <em>age, annual income, equity</em>, <span class="math notranslate nohighlight">\(3+1=4\)</span> neurons are required at the input (3 neurons <span class="math notranslate nohighlight">\(x_1,x_2,x_3\)</span> for the 3 features plus the constant bias <span class="math notranslate nohighlight">\(x_0=1\)</span>) and one neuron is required at the output. For non-numeric features at the input, the number of neurons in the inut-layer is not directly given by the number of features, since each non-numeric feature must be <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">One-Hot encoded</a> before passing it to the Neural Network.</p>
<p>Inbetween the input- and the output-layer there may be zero, one or more other layers. The number of layers in a Neural Network is an essential architectural hyperparameter. <strong>Hyperparameters</strong> in Neural Networks, as well as in all other Machine Learning algorithms, are parameters, which are not learned automatically in the training phase, but must be configured from outside. Finding appropriate hyperparameters for the given task and the given data is possibly the most challenging task in machine-learning.</p>
</div>
<div class="section" id="feedforward-and-recurrent-neural-networks">
<h3><span class="section-number">10.1.3.2. </span>Feedforward- and Recurrent Neural Networks<a class="headerlink" href="#feedforward-and-recurrent-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>In <strong>Feedforward Neural Networks (FNN)</strong> signals are propagated only in one direction - from the input- towards the output layer. In a network with <span class="math notranslate nohighlight">\(L\)</span> layers, the input-layer is typically indexed by 0 and the output-layer’s index is <span class="math notranslate nohighlight">\(L\)</span> (as mentioned above the input-layer is ignored in the layer-count). Then in a FNN the output of layer <span class="math notranslate nohighlight">\(j\)</span> can be passed to the input of neurons in layer <span class="math notranslate nohighlight">\(i\)</span>, if and only if <span class="math notranslate nohighlight">\(i&gt;j\)</span>.</p>
<p><strong>Recurrent Neural Networks (RNN)</strong>, in contrast to FNNs, not only have forward connections, but also backward-connections. I.e the output of neurons in layer <span class="math notranslate nohighlight">\(j\)</span> can be passed to the input of neurons in the same layer or to neurons in layers of index <span class="math notranslate nohighlight">\(k&lt;j\)</span>.</p>
</div>
<div class="section" id="general-concept-of-supervised-learning">
<h3><span class="section-number">10.1.3.3. </span>General Concept of Supervised Learning<a class="headerlink" href="#general-concept-of-supervised-learning" title="Permalink to this headline">¶</a></h3>
<p>Neural Networks can be applied for supervised and unsupervised learning. By far the most applications apply Neural Networks for <strong>supervised learning</strong> for classification or regression. This notebook only considers this case. Neural Networks for unsupervised learning would be for example <a class="reference external" href="https://en.wikipedia.org/wiki/Self-organizing_map">Self Organizing Maps</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Autoencoder">Auto Encoders</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">Restricted Boltzmann Machines</a>.</p>
<p>The general concept of supervised learning of a neural network is sketched in the picture below.</p>
<p><img alt="Principle of Learning" src="https://maucher.home.hdm-stuttgart.de/Pics/learnGradientDescent.png" /></p>
<p>In supervised learning each training element is a pair of input/target. The input contains the observable features, and the target is either the true class-label in the case of classification or the true numeric output value in the case of regression. A Neural Network is trained by passing a single training-element to the network. For the given input the output of the network is calculated, based on the current weight values. This output of the network is compared with the target. As long as there is a significant difference between the output and the target, the weights of the networks are adapted.
In a well trained network, the deviation between output and target is as small as possible for all training-elements.</p>
</div>
<div class="section" id="gradient-descent-and-stochastic-gradient-descent-learning">
<h3><span class="section-number">10.1.3.4. </span>Gradient Descent and Stochastic Gradient Descent Learning<a class="headerlink" href="#gradient-descent-and-stochastic-gradient-descent-learning" title="Permalink to this headline">¶</a></h3>
<p>In the previous chapters it was described how to apply SLPs for linear regression and linear classification. Moreover, in section <a class="reference external" href="#learnconcept">Concept of Learning</a> the general idea of training a Neural Network has been presented. Now, a concrete realization of this general idea is presented - <strong>Gradient Descent -</strong> and <strong>Stochastic Gradient Descent Learning</strong>. This approach is not only applied for all types of Neural Networks, but for many other supervised Machine Learning algorithms.</p>
</div>
<div class="section" id="gradient-descent-learning">
<h3><span class="section-number">10.1.3.5. </span>Gradient Descent Learning<a class="headerlink" href="#gradient-descent-learning" title="Permalink to this headline">¶</a></h3>
<p>The concept of Gradient Descent Learning is as follows:</p>
<ol class="simple">
<li><p>Define a <strong>Loss Function</strong> <span class="math notranslate nohighlight">\(E(T,\Theta)\)</span>, which somehow measures the deviation between the current network <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> output and the target output <span class="math notranslate nohighlight">\(\mathbf{r}\)</span>. As above,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[T=\lbrace(x_{1,t},x_{2,t},\ldots,x_{d,t}),r_t \rbrace_{t=1}^N,\]</div>
<p>is the set of labeled training data and</p>
<div class="math notranslate nohighlight">
\[\Theta=\lbrace W_{1,0},W_{1,1},\ldots, W_{K,d+1} \rbrace\]</div>
<p>is the set of parameters (weights), which are adapted during training.
2. Calculate the gradient of the Loss Function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla E(T,\Theta) = \left( \begin{array}{c}  \frac{\partial E}{\partial W_{1,0}} \\ \frac{\partial E}{\partial W_{1,1}} \\ \vdots \\  \frac{\partial E}{\partial W_{K,d+1}} \end{array} \right). \end{split}\]</div>
<p>The gradient of a function points towards the steepest ascent of the function at the point, where it is calculated. The negative gradient <span class="math notranslate nohighlight">\(-\nabla E(T,\Theta)\)</span> points towards the steepest descent of the function.
3. Adapt all parameters into the direction of the negative gradient. This weight adaptation guarantees that the Loss Function is iteratively minimized.:</p>
<div class="math notranslate nohighlight">
\[W_{i,j}=W_{i,j}+\Delta W_{i,j} = W_{i,j}+\eta \cdot -\frac{\partial E}{\partial W_{i,j}},\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\)</span> is the important hyperparameter <strong>learning rate</strong>. The learning rate controls the step-size of weight adaptations. A small <span class="math notranslate nohighlight">\(\eta\)</span> implies that weights are adapted only slightly per iteration and the learning algorithm converges slowly. A large learning-rate implies strong adaptations per iteration. However, in this case the risk of <em>jumping over the minimum</em> is increased. Typical values for <span class="math notranslate nohighlight">\(\eta\)</span> are in the range of <span class="math notranslate nohighlight">\([0.0001,0.1]\)</span>.</p>
<p><img alt="Gradient Descent Flowchart" src="https://maucher.home.hdm-stuttgart.de/Pics/peaksexampleboth.jpg" /></p>
</div>
</div>
<div class="section" id="single-layer-perceptron">
<h2><span class="section-number">10.1.4. </span>Single Layer Perceptron<a class="headerlink" href="#single-layer-perceptron" title="Permalink to this headline">¶</a></h2>
<p>A Single Layer Perceptron (SLP) is a Feedforward Neural Network (FNN), which consists only of an input- and an output layer (the output-layer is the <em>single</em> layer). All neurons of the input layer are connected to all neurons of the output layer. A layer with this property is also called a <strong>fully-connected layer</strong> or a <strong>dense layer</strong>. SLPs can be applied to learn</p>
<ul class="simple">
<li><p>a linear binary classifier</p></li>
<li><p>a linear classifier for more than 2 classes</p></li>
<li><p>a linear regression model</p></li>
</ul>
<div class="section" id="slp-for-regression">
<h3><span class="section-number">10.1.4.1. </span>SLP for Regression<a class="headerlink" href="#slp-for-regression" title="Permalink to this headline">¶</a></h3>
<p>A SLP can be applied to learn a linear function</p>
<div class="math notranslate nohighlight">
\[y=f(x_1,x_2,\ldots,x_d)\]</div>
<p>from a set of N supervised observations</p>
<div class="math notranslate nohighlight">
\[T=\lbrace(x_{1,t},x_{2,t}, ,x_{d,t}),r_t \rbrace_{t=1}^N,\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{j,t}\)</span> is the j.th feature of the t.th training-element and <span class="math notranslate nohighlight">\(r_t\)</span> is the numeric target value of the t.th training-element.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpW0.png" width="350" class="center"><p>As depicted above, for linear regression only a <strong>single neuron in the output-layer</strong> is required. The activation function <span class="math notranslate nohighlight">\(g()\)</span> applied for regression is the <strong>identity function</strong>. The loss-function, which is minimized in the training procedure is the <strong>sum of squared error</strong>:</p>
<div class="math notranslate nohighlight">
\[SSE(T,\Theta)= \frac{1}{2} \sum\limits_{t=1}^N (r_t-y_t)^2 = \frac{1}{2} \sum\limits_{t=1}^N \left( r_t-\sum\limits_{j=0}^d w_j x_{j,t}\right)^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\Theta=\lbrace w_0,w_1,\ldots, w_d \rbrace\)</span> is the set of weights, which are adapted in the training process.</p>
<p><strong>Example:</strong> The <span class="math notranslate nohighlight">\(N=5\)</span> training-elements given in the table of the picture below contain only a single input feature <span class="math notranslate nohighlight">\(x_1\)</span> and the corresponding target-value <span class="math notranslate nohighlight">\(r\)</span>. From these training-elements a SLP can learn a linear function <span class="math notranslate nohighlight">\(y=w_0+w_1 x_1\)</span>, which minimizes the loss-function SSE.</p>
<p><img alt="Linear Regression" src="https://maucher.home.hdm-stuttgart.de/Pics/slp1dimlinearregression.png" /></p>
</div>
<div class="section" id="slp-for-binary-classification">
<h3><span class="section-number">10.1.4.2. </span>SLP for binary classification<a class="headerlink" href="#slp-for-binary-classification" title="Permalink to this headline">¶</a></h3>
<p>A SLP can be applied to learn a binary classifier from a set of N labeled observations</p>
<div class="math notranslate nohighlight">
\[T=\lbrace(x_{1,t},x_{2,t},\ldots,x_{d,t}),r_t \rbrace_{t=1}^N,\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{j,t}\)</span> is the j.th feature of the t.th training-element and <span class="math notranslate nohighlight">\(r_t \in \lbrace 0,1 \rbrace\)</span> is the class-index of the t.th training-element.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpW0.png" width="350" class="center"><p>As depicted above, for binary classification only a <strong>single neuron in the output-layer</strong> is required. The activation function <span class="math notranslate nohighlight">\(g()\)</span> applied for binary classification is either the <strong>threshold-</strong> or the <strong>sigmoid-function</strong>. The threshold-function output values are either 0 or 1, i.e. this function can provide only a <em>hard</em> classifikcation-decision, with no further information on the certainty of this decision. In contrast the value range of the sigmoid-function covers all floats between 0 and 1. It can be shown that if the weighted-sum is processed by the sigmoid-function the output is an indicator for the a-posteriori propability that the given observation belongs to class <span class="math notranslate nohighlight">\(C_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(C_1|(x_{1},x_{2}, ,x_{d}))=1-P(C_0|(x_{1},x_{2},\ldots,x_{d})).\]</div>
<p>If the output value</p>
<div class="math notranslate nohighlight">
\[y=sigmoid(\sum\limits_{j=0}^d w_j x_{j,t})\]</div>
<p>is larger than 0.5 the observation <span class="math notranslate nohighlight">\((x_{1},x_{2}, \ldots,x_{d})\)</span> is assigned to class <span class="math notranslate nohighlight">\(C_1\)</span>, otherwise it is assigned to class <span class="math notranslate nohighlight">\(C_0\)</span>. A value close to 0.5 indicates an uncertaion decision, whereas a value close to 0 or 1 indicates a certain decision.</p>
<p>In the case that the sigmoid-activation function is applied, the loss-function, which is minimized in the training procedure is the <strong>binary cross-entropy function</strong>:</p>
<div class="math notranslate nohighlight">
\[L(T,\Theta)=  \sum\limits_{t=1}^N r_{t} \log y_{t}+(1-r_{t}) \log(1-y_{t}),\]</div>
<p>where <span class="math notranslate nohighlight">\(r_t\)</span> is the target class-index and <span class="math notranslate nohighlight">\(y_t\)</span> is the output of the sigmoid-function, for the t.th training-element. Again, <span class="math notranslate nohighlight">\(\Theta=\lbrace w_0,w_1,\ldots, w_d \rbrace\)</span> is the set of weights, which are adapted in the training process.</p>
<p><strong>Example:</strong> The <span class="math notranslate nohighlight">\(N=9\)</span> 2-dimensional labeled training-elements given in the table of the picture below are applied to learn a SLP for binary classification. The learned model can be specified by the parameters (weights)</p>
<div class="math notranslate nohighlight">
\[w_0=-3, w_1=0.6, w_2=1.\]</div>
<p>These weights define a line</p>
<div class="math notranslate nohighlight">
\[w_0+w_1x_1+w_2x_2=0 \Longrightarrow x_2 = -\frac{w_1}{w_2}x_1 -\frac{w_0}{w_2} ,\]</div>
<p>whose slope is</p>
<div class="math notranslate nohighlight">
\[m=-\frac{w_1}{w_2}=-0.6\]</div>
<p>and whose intersection with the <span class="math notranslate nohighlight">\(x_2\)</span>-axis is</p>
<div class="math notranslate nohighlight">
\[
b=-\frac{w_0}{w_2}=3. 
\]</div>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpBinaryClassification.png" style="width: 600px;">
<p>Once this model, i.e. the set of weights, is learned it can be applied for classification as follows: A new observation <span class="math notranslate nohighlight">\(\mathbf{x'}=(x'_1,x'_2)\)</span> is inserted into the learned equation <span class="math notranslate nohighlight">\(w_0 \cdot 1 + w_1 \cdot x'_1 + w_2 \cdot x'_2\)</span>. The result of this linear equation is passed to the sigmoid-function. If sigmoid-function’s output is <span class="math notranslate nohighlight">\(&gt;0.5\)</span> the most probable class is <span class="math notranslate nohighlight">\(C_1\)</span>, otherwise it is <span class="math notranslate nohighlight">\(C_0\)</span>.</p>
</div>
<div class="section" id="slp-for-classification-in-k-2-classes">
<h3><span class="section-number">10.1.4.3. </span>SLP for classification in <span class="math notranslate nohighlight">\(K&gt;2\)</span> classes<a class="headerlink" href="#slp-for-classification-in-k-2-classes" title="Permalink to this headline">¶</a></h3>
<p>A SLP can be applied to learn a non-binary classifier from a set of N labeled observations</p>
<div class="math notranslate nohighlight">
\[T=\lbrace(x_{1,t},x_{2,t}, \ldots, x_{d,t}),r_t \rbrace_{t=1}^N,\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{j,t}\)</span> is the j.th feature of the t.th training-element and <span class="math notranslate nohighlight">\(r_t \in \lbrace 0,1 \rbrace\)</span> is the class-index of the t.th training-element.
<a id="slpmulitclass"></a>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpK3B.png" width="450" class="center"></p>
<p>As depicted above, for classification into <span class="math notranslate nohighlight">\(K&gt;2\)</span> classes <strong>K neurons are required in the output-layer</strong>. The activation function <span class="math notranslate nohighlight">\(g()\)</span> applied for non-binary classification is usually the <strong>softmax-function</strong>:</p>
<div class="math notranslate nohighlight">
\[g(in_i,in_j)=\frac{\exp(in_i)}{\sum\limits_{j=1}^{K} \exp(in_j)} \quad with \quad in_j=\sum\limits_{j=0}^d w_j x_{j,t}\]</div>
<p>The softmax-function outputs for for each neuron in the output-layer a value <span class="math notranslate nohighlight">\(y_k\)</span>, with the property, that</p>
<div class="math notranslate nohighlight">
\[\sum\limits_{k=1}^K y_k = 1.\]</div>
<p>Each of these outputs is an indicator for the
a-posteriori propability that the given observation belongs to class <span class="math notranslate nohighlight">\(C_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(C_i|(x_{1},x_{2}, \ldots,x_{d})).\]</div>
<p>The class, whose neuron outputs the maximum value is the most likely class for the current observation at the input of the SLP.</p>
<p>In the case that the softmax-activation function is applied, the loss-function, which is minimized in the training procedure is the <strong>cross-entropy function</strong>:</p>
<div class="math notranslate nohighlight">
\[L(T,\Theta)= \sum\limits_{t=1}^N \sum\limits_{k=1}^K r_{t,k} \log(y_{t,k}),\]</div>
<p>where <span class="math notranslate nohighlight">\(\Theta=\lbrace w_0,w_1,\ldots, w_d \rbrace\)</span> is the set of weights, which are adapted in the training process. <span class="math notranslate nohighlight">\(r_{t,k}=1\)</span>, if the t.th training-element belongs to class <span class="math notranslate nohighlight">\(k\)</span>, otherwise it is 0. <span class="math notranslate nohighlight">\(y_{t,k}\)</span> is the output of the k.th neuron for the t.th training-element.</p>
<p>Each output neuron has its own set of weights, and each weight-set defines a (d-1)-dimensional hyperplane in the d-dimensional space. However, now these hyperplanes are not the class boundary itself, but they determine the class boundaries, which are actually of convex shape as depicted below. In the picture below, the red area indicates the inputs, who yield a maximum output at the neuron, whose weights belong to the red line, the blue area is the of inputs, whose maximum value is at the neuron, which belongs to the blue line and the green area comprises the inputs, whose maximum value is at the neuron, which belongs to the green line.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpMultiClassRegions.png" width="400" class="center"></div>
</div>
<div class="section" id="summary-single-layer-perceptron">
<h2><span class="section-number">10.1.5. </span>Summary Single Layer Perceptron<a class="headerlink" href="#summary-single-layer-perceptron" title="Permalink to this headline">¶</a></h2>
<p><img alt="SLP summary" src="https://maucher.home.hdm-stuttgart.de/Pics/slpSummary.png" /></p>
</div>
<div class="section" id="multi-layer-perceptron">
<h2><span class="section-number">10.1.6. </span>Multi Layer Perceptron<a class="headerlink" href="#multi-layer-perceptron" title="Permalink to this headline">¶</a></h2>
<div class="section" id="notations-and-basic-characteristics">
<h3><span class="section-number">10.1.6.1. </span>Notations and Basic Characteristics<a class="headerlink" href="#notations-and-basic-characteristics" title="Permalink to this headline">¶</a></h3>
<p>A Multi Layer Perceptron (MLP) with <span class="math notranslate nohighlight">\(L\geq 2\)</span> layers is a Feedforward Neural Network (FNN), which consists of</p>
<ul class="simple">
<li><p>an input-layer (which is actually not counted as <em>layer</em>)</p></li>
<li><p>an output layer</p></li>
<li><p>a sequence of <span class="math notranslate nohighlight">\(L-1\)</span> hidden layers inbetween the input- and output-layer</p></li>
</ul>
<p>Usually the number of hidden layers is 1,2 or 3. All neurons of a layer are connected to all neurons of the successive layer. A layer with this property is also called a <strong>fully-connected layer</strong> or a <strong>dense layer</strong>.</p>
<p>An example of a <span class="math notranslate nohighlight">\(L=3\)</span> layer MLP is shown in the following picture.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/mlpL3.png" alt="Drawing" style="width: 600px;">
<p>As in the case of SLPs, the biases in MLP can be modelled implicitily by including to all non-output-layers a constant neuron <span class="math notranslate nohighlight">\(x_0=1\)</span>, or by the explicit bias-vector <span class="math notranslate nohighlight">\(\mathbf{b^l}\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>. In the picture above, the latter option is applied.</p>
<p>In order to provide a unified description the following notation is used:</p>
<ul class="simple">
<li><p>the number of neurons in layer <span class="math notranslate nohighlight">\(l\)</span> is denoted by <span class="math notranslate nohighlight">\(z_l\)</span>.</p></li>
<li><p>the output of the layer in depth <span class="math notranslate nohighlight">\(l\)</span> is denoted by the vector <span class="math notranslate nohighlight">\(\mathbf{h^l}=(h_1^l,h_2^l,\ldots,h_{z_l}^l)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}=\mathbf{h^0}\)</span> is the input to the network,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}=\mathbf{h^L}\)</span> is the network’s output,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b^l}\)</span> is the bias-vector of layer <span class="math notranslate nohighlight">\(l\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(W^l\)</span> is the weight-matrix of layer <span class="math notranslate nohighlight">\(l\)</span>. It’s entry <span class="math notranslate nohighlight">\(W_{ij}^l\)</span> is the weight from the j.th neuron in layer <span class="math notranslate nohighlight">\(l-1\)</span> to the i.th neuron in layer <span class="math notranslate nohighlight">\(l\)</span>. Hence, the weight-matrix <span class="math notranslate nohighlight">\(W^l\)</span> has <span class="math notranslate nohighlight">\(z_l\)</span> rows and <span class="math notranslate nohighlight">\(z_{l-1}\)</span> columns.</p></li>
</ul>
<p>With this notation the <strong>Forward-Pass</strong> of the MLP in the picture above can be calculated as follows:</p>
<p><strong>Output of first hidden-layer:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\left( \begin{array}{c} h_1^1 \\ h_2^1 \\ h_3^1 \\ h_4^1 \end{array} \right) = g\left( \left( \begin{array}{ccc} W_{11}^1 &amp; W_{12}^1 &amp; W_{13}^1 \\ W_{21}^1 &amp; W_{22}^1 &amp; W_{23}^1 \\ W_{31}^1 &amp; W_{32}^1 &amp; W_{33}^1 \\ W_{41}^1 &amp; W_{42}^1 &amp; W_{43}^1 \end{array} \right) \left( \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right) + \left( \begin{array}{c} b_1^1 \\ b_2^1 \\ b_3^1 \\ b_4^1 \end{array} \right) \right)\end{split}\]</div>
<p><strong>Output of second hidden-layer:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\left( \begin{array}{c} h_1^2 \\ h_2^2 \\ h_3^2 \end{array} \right) = g\left( \left( \begin{array}{cccc} W_{11}^2 &amp; W_{12}^2 &amp; W_{13}^2 &amp; W_{14}^2\\ W_{21}^2 &amp; W_{22}^2 &amp; W_{23}^2 &amp; W_{24}^2\\ W_{31}^2 &amp; W_{32}^2 &amp; W_{33}^2 &amp; W_{34}^2 \end{array} \right) \left( \begin{array}{c} h^1_1 \\ h^1_2 \\ h^1_3 \\ h^1_4 \end{array} \right) + \left( \begin{array}{c} b_1^2 \\ b_2^2 \\ b_3^2 \end{array} \right) \right)\end{split}\]</div>
<p><strong>Output of the network:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}y = \left( \begin{array}{c} h_1^3 \\ \end{array} \right) = g\left( \left( \begin{array}{ccc} W_{11}^3 &amp; W_{12}^3 &amp; W_{13}^3 \end{array} \right) \left( \begin{array}{c} h^2_1 \\ h^2_2 \\ h^2_3 \end{array} \right) + \left( \begin{array}{c} b_1^3 \end{array} \right) \right)\end{split}\]</div>
<p>As in the case of Single Layer Perceptrons the three categories</p>
<ul class="simple">
<li><p>regression,</p></li>
<li><p>binary classification</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span>-ary classification</p></li>
</ul>
<p>are distinguished. The corresponding MLP output-layer is the same as in the case of a SLP.</p>
<p>In contrast to SLPs, MLPs are able to <strong>learn non-linear</strong> models. This difference is depicted below: The left hand side shows the linear classification-boundary, as learned by a SLP, whereas on the right-hand side the non-linear boundary, as learned by a MLP from the same training data, is plotted.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/nonlinearClassification.png" alt="Drawing" style="width: 800px;"/></div>
</div>
<div class="section" id="early-mlp-example-autonomos-driving">
<h2><span class="section-number">10.1.7. </span>Early MLP Example: Autonomos Driving<a class="headerlink" href="#early-mlp-example-autonomos-driving" title="Permalink to this headline">¶</a></h2>
<p>The ALVINN net is a MLP with one hidden layer. It has been designed and trained for <em>road following</em> in autonomous driving. The input has been provided by a simple <span class="math notranslate nohighlight">\(30 \times 32\)</span> greyscale camera. As shown in the picture below, the hidden layer contains only 4 neurons. In the output-layer each of the 30 neurons belongs to one “steering-wheel-direction”. The training data has been collected by recording videos while an expert driver steers the car. For each frame (input) the steering-wheel-direction (label) has been tracked.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/alvinnNN.jpg" width=450 class="center">
<p>After training the vehicle cruised autonomously for 90 miles on a highway at a speed of up to 70mph. The test-highway has not been included in the training cruises.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./07neuralnetworks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="07neuralnetworks.html" title="previous page"><span class="section-number">10. </span>Neural Networks</a>
    <a class='right-next' id="next-link" href="02RecurrentNeuralNetworks.html" title="next page"><span class="section-number">10.2. </span>Recurrent Neural Networks</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>