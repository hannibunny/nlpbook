
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10.2. Recurrent Neural Networks &#8212; Natural Language Processing Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10.3. Convolutional Neural Networks" href="03ConvolutionNeuralNetworks.html" />
    <link rel="prev" title="10.1. Neural Networks Introduction" href="01NeuralNets.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Natural Language Processing Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01access/01access.html">
   1. Access and Preprocess Text
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/01AccessTextFromFile.html">
     1.1. Access and Analyse Contents of Textfiles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/02crawlHTMLcontent.html">
     1.2. Access Contents of HTML Page
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/03crawlWebSites.html">
     1.3. Download HTML Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/04crawlRSSFeeds.html">
     1.4. Access RSS Feed
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/05RegularExpressions.html">
     1.5. Regular expressions in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/06accessTweets.html">
     1.6. Access Tweets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02normalisation/02normalisation.html">
   2. Word Normalisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/01morphology.html">
     2.1. Morphology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/03StemLemma.html">
     2.2. TextBlob Stemming and Lemmatization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/01LevenstheinDistance.html">
     2.3. Correction of Spelling Errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03postagging/03postagging.html">
   3. Part-Of-Speech Tagging
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03postagging/01tagsetsAndAlgorithms.html">
     3.1. PoS Tagsets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03postagging/02PosTagging.html">
     3.3. POS Tagging with NLTK
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04ngram/04ngram.html">
   4. N-Gram Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05representations/05representations.html">
   5. Vector Representations of Words and Documents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05representations/01WordEmbeddingImplementation.html">
   6. Applying Word-Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05representations/02gensimDocModelSimple.html">
   7. Document models and similarity
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05representations/05topicextraction.html">
   8. Topic Extraction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05representations/05lsi.html">
     8.1. Latent Semantic Indexing (LSI)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05representations/02LatentSemanticIndexing.html">
     8.2. Implementation of Topic Extraction and Document Clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06classification/06classification.html">
   9. Text Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/06classificationMetrics.html">
     9.1. Validation of Classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/07classificationNaiveBayes.html">
     9.2. Naive Bayes Text Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/FakeNewsClassification.html">
     9.3. Text Classification Application: Fake News detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="07neuralnetworks.html">
   10. Neural Networks
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="01NeuralNets.html">
     10.1. Neural Networks Introduction
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     10.2. Recurrent Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03ConvolutionNeuralNetworks.html">
     10.3. Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04CNN.html">
     10.4. CNN, LSTM and Attention for IMDB Movie Review classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   11. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/07neuralnetworks/02RecurrentNeuralNetworks.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/07neuralnetworks/02RecurrentNeuralNetworks.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recap-feedforward-neural-networks">
   10.2.1. Recap: Feedforward Neural Networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   10.2.2. Recurrent Neural Networks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#application-categories-of-recurrent-neural-networks">
     10.2.2.1. Application Categories of Recurrent Neural Networks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-to-one-feedforward-neural-network">
       10.2.2.1.1. One-to-One (Feedforward Neural Network)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#many-to-one-rnn">
       10.2.2.1.2. Many-to-One (RNN)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#many-to-many-rnn">
       10.2.2.1.3. Many-to-Many (RNN)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#one-to-many-rnn">
       10.2.2.1.4. One-To-Many (RNN)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bidirectional-rnn">
       10.2.2.1.5. Bidirectional RNN
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#long-short-term-memory-lstm-networks">
   10.2.3. Long-short-Term-Memory (LSTM) networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gated-recurrent-unit-gru">
   10.2.4. Gated Recurrent Unit (GRU)
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="recurrent-neural-networks">
<h1><span class="section-number">10.2. </span>Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Author: Johannes Maucher</p></li>
<li><p>Last Update: 30.10.2020</p></li>
</ul>
<div class="section" id="recap-feedforward-neural-networks">
<h2><span class="section-number">10.2.1. </span>Recap: Feedforward Neural Networks<a class="headerlink" href="#recap-feedforward-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>Feedforward neural networks have already been introduced in <a class="reference internal" href="01NeuralNets.html"><span class="doc std std-doc">01NeuralNets.ipynb</span></a>. Here, we just repeat the basics of feedforward nets in order to clarify how Recurrent Neural Networks differ from them.</p>
<p>In artificial neural networks the neurons are typically arranged in layers. There exist weighted connections between the neurons in different layers. The weights are learned during the training-phase and define the overall function <span class="math notranslate nohighlight">\(f\)</span>, which maps an input-signal <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to an output <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. In a feed-forward network the neural layers are  ordered such that the signals of the input-neurons are connected to the input of the neurons in the first hidden layer. The output of the neurons in the <span class="math notranslate nohighlight">\(i.th\)</span> hidden layer are connected to the input of the neurons in the <span class="math notranslate nohighlight">\((i+1).th\)</span> hidden layer and so on. The output of the neurons in the last hidden layer, are connected to the neurons in the output layer. In particular there are no backward-connections in a feed-forward network. A multilayer-perceptron (MLP) is a feed-forward network in which all neurons in layer <span class="math notranslate nohighlight">\(i\)</span> are connected to all neurons in layer <span class="math notranslate nohighlight">\(i+1\)</span>. A simple MLP with 3 input neurons, one hidden layer and one neuron in the output layer is depicted in figure below.</p>
<p><img alt="mlp" src="http://maucher.home.hdm-stuttgart.de/Pics/mlp.PNG" /></p>
<p>The picture below contains another representation of the MLP, which hides the details of the topology, while emphasizing the algebraic operations:</p>
<p><img alt="abstractmlp" src="http://maucher.home.hdm-stuttgart.de/Pics/abstrmlp.png" /></p>
<p>If the output of the neurons in layer <span class="math notranslate nohighlight">\(k\)</span> are denoted by</p>
<div class="math notranslate nohighlight">
\[
\mathbf{h}^k=(h_1^k,h_2^k,\ldots,h_{z_k}^k),
\]</div>
<p>the input to the network is <span class="math notranslate nohighlight">\(\mathbf{h}^0=\mathbf{x}\)</span> and the bias values of the <span class="math notranslate nohighlight">\(z_k\)</span> neurons in layer <span class="math notranslate nohighlight">\(k\)</span> are arranged in the vector</p>
<div class="math notranslate nohighlight">
\[
\mathbf{b}^k=(b_1^k,b_2^k,\ldots,b_{z_k}^k),
\]</div>
<p>then the output at each layer <span class="math notranslate nohighlight">\(k\)</span> can be calculated by</p>
<div class="math notranslate nohighlight">
\[
\mathbf{h}^k = g(\mathbf{b}^k + W^k \mathbf{h}^{k-1}), 
\]</div>
<p>where <span class="math notranslate nohighlight">\(g(\cdot)\)</span> is the <em>activation-function</em>. Typical activation-functions are e.g. sigmoid-, tanh-, softmax- or the identity-function. The weight matrix <span class="math notranslate nohighlight">\(W^k\)</span> of layer <span class="math notranslate nohighlight">\(k\)</span> consists of <span class="math notranslate nohighlight">\(z_k\)</span> rows and <span class="math notranslate nohighlight">\(z_{k-1}\)</span> columns. The entry <span class="math notranslate nohighlight">\(W_{ij}^k\)</span> in row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span> is the weight of the connection from neuron <span class="math notranslate nohighlight">\(j\)</span> in layer <span class="math notranslate nohighlight">\(k-1\)</span> to neuron <span class="math notranslate nohighlight">\(i\)</span> in layer <span class="math notranslate nohighlight">\(k\)</span>.</p>
</div>
<div class="section" id="id1">
<h2><span class="section-number">10.2.2. </span>Recurrent Neural Networks<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>In feed-forward neural networks the output depends only on the network parameters and the current input vector. Previous or successive input-vectors are not regarded. This means, that <strong>feed-forward networks do not model correlations between successive input-vectors</strong>.</p>
<p>E.g. in Natural Language Processing (NLP) the input signals are often words of a phrase, sentence, paragraph or document. Obviously, successive words are correlated. Speech and text are not the only domains of <strong>correlated data</strong>. For all <strong>time-series-data</strong>, e.g. temperature, stock-market, unemployment-numbers, …we also have temporal correlations. A feed-forward network would ignore these correlations. Recurrent Neural Networks (RNNs) would be better for this type of data.</p>
<p>The architecture of a <strong>simple recurrent layer</strong> is depicted in the figure below. RNNs operate on variable-length sequences of input. In contrast to simple feedforward-networks, they have connections in forward- and backward-direction. The backward-connections realise an internal state, or memory. In each time stamp the output <span class="math notranslate nohighlight">\(h^1(t)\)</span> is calculated in dependence of the current input <span class="math notranslate nohighlight">\(x(t)\)</span> and the previous output <span class="math notranslate nohighlight">\(h^1(t-1)\)</span>. Thus the current output depends on the current input and all former outputs. In this way RNNs model correlations between successive input elements.</p>
<p><img alt="RNN" src="http://maucher.home.hdm-stuttgart.de/Pics/rnn.png" /></p>
<p><a id=abstrnn></a>
<img alt="abstrRNN" src="http://maucher.home.hdm-stuttgart.de/Pics/abstrrnn.png" /></p>
<p>The recurrent-layer’s output <span class="math notranslate nohighlight">\(\mathbf{h}^1(t)\)</span> from the current input <span class="math notranslate nohighlight">\(\mathbf{x}(t)\)</span> and the previous output <span class="math notranslate nohighlight">\(\mathbf{h}^1(t-1)\)</span>, can equivalently be realized by a single matrix multiplication, if the weight matrices <span class="math notranslate nohighlight">\(W^1 \)</span> and <span class="math notranslate nohighlight">\(R^{1}\)</span> are stacked horizontally and the column-vectors <span class="math notranslate nohighlight">\(\mathbf{x}(t)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{h}^1(t-1)\)</span> are stacked vertically:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{h}^1(t) = g\left(W^1 \mathbf{x}(t)+R^{1} \mathbf{h}^1(t-1)+\mathbf{b}^1\right) 
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
= g\left( (W^1 \mid R^{1}) \left(\begin{array}{c}\mathbf{x}(t) \\ \mathbf{h}^1(t-1) \end{array} \right)+\mathbf{b}^1 \right)
\end{split}\]</div>
<p>For simple recurrent layers of this type, typically the <strong>tanh activation function is applied</strong>.</p>
<p>The picture above depicts a single recurrent layer. In a (deep) neural network several recurrent layers can be stacked togehter. A convenient architecture-type for sequence-classification (e.g. text-classification) contains one or more recurrent layers and one or more dense layers at the output. In this constellation the dense layers at the output serve as classifier and the recurrent layers at the input generate a meaningful representation of the input-sequence. However, sequence-classification (<em>many-to-one</em>) is only one application category of recurrent neural networks. Other categories are described in <a class="reference external" href="#appcat">subsection Application Categories</a>.</p>
<p><a id="appcat"></a></p>
<div class="section" id="application-categories-of-recurrent-neural-networks">
<h3><span class="section-number">10.2.2.1. </span>Application Categories of Recurrent Neural Networks<a class="headerlink" href="#application-categories-of-recurrent-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>In order to distinguish RNN application categories we apply an abstract representations of neural networks in which</p>
<ul class="simple">
<li><p>the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is represented by a  <strong><font color='green'>green rectangle</font></strong>.</p></li>
<li><p>the output <span class="math notranslate nohighlight">\(\mathbf{h}^k\)</span> of a (recurrent) hidden-layer is represented by a <strong><font color='purple'>purple rectangle</font></strong>.</p></li>
<li><p>the output of the network <span class="math notranslate nohighlight">\(\mathbf{y}=\mathbf{h}^L\)</span> is represented by a <strong><font color='red'>red rectangle</font></strong>.</p></li>
</ul>
<p>Moreover, without loss of generality, only networks with a single (recurrent) hidden layer are considered here.</p>
<blockquote>
<div><p><strong>Note:</strong> The application categories, listed below, are not only applicable for simple RNNs, but also for <a class="reference external" href="#lstm">LSTMs</a> and <a class="reference external" href="#gru">GRUs</a>, which will be described in the following sections.</p>
</div></blockquote>
<div class="section" id="one-to-one-feedforward-neural-network">
<h4><span class="section-number">10.2.2.1.1. </span>One-to-One (Feedforward Neural Network)<a class="headerlink" href="#one-to-one-feedforward-neural-network" title="Permalink to this headline">¶</a></h4>
<p>With the notation defined above a Feedworward Neural Network with one hidden layer is abstractly defined as below:</p>
<p><img alt="oneToOne" src="http://maucher.home.hdm-stuttgart.de/Pics/oneToOne.PNG" /></p>
<p>This type of model represents the standard supervised learning algorithms for classification and regression, where all input-vectors are considered to be independent of each other.</p>
</div>
<div class="section" id="many-to-one-rnn">
<h4><span class="section-number">10.2.2.1.2. </span>Many-to-One (RNN)<a class="headerlink" href="#many-to-one-rnn" title="Permalink to this headline">¶</a></h4>
<p>A many-to-one recurrent architecture maps a sequence of input-vectors to a single output. An example for this category is <strong>text-classification</strong>, where each input represents a single word and a class-label must be calculated for the entire sequence. At each time stamp <span class="math notranslate nohighlight">\(t \in [1,T]\)</span></p>
<ul class="simple">
<li><p>a new input-vector <span class="math notranslate nohighlight">\(x(t)\)</span> (word representation) is passed to the net</p></li>
<li><p>a new hidden-state <span class="math notranslate nohighlight">\(h(t)\)</span> is calculated in dependence of <span class="math notranslate nohighlight">\(x(t)\)</span> and the previous hidden-state <span class="math notranslate nohighlight">\(h(t-1)\)</span></p></li>
</ul>
<p>At the end of the sequence the network output <span class="math notranslate nohighlight">\(y\)</span> is calculated from the hidden-state <span class="math notranslate nohighlight">\(h(T)\)</span>.</p>
<p><img alt="manyToOne" src="http://maucher.home.hdm-stuttgart.de/Pics/manyToOne.PNG" /></p>
</div>
<div class="section" id="many-to-many-rnn">
<h4><span class="section-number">10.2.2.1.3. </span>Many-to-Many (RNN)<a class="headerlink" href="#many-to-many-rnn" title="Permalink to this headline">¶</a></h4>
<p>A many-to-many recurrent architecture maps a sequence of input-vectors to a sequence of output-vectors. An example for this category is <strong>language-translation</strong>, where each input represents a single word. The entire sequence is usually a sentence and the output is the translated sentence. At each time stamp <span class="math notranslate nohighlight">\(t \in [1,T]\)</span></p>
<ul class="simple">
<li><p>a new input-vector <span class="math notranslate nohighlight">\(x(t)\)</span> (word representation) is passed to the net</p></li>
<li><p>a new hidden-state <span class="math notranslate nohighlight">\(h(t)\)</span> is calculated in dependence of <span class="math notranslate nohighlight">\(x(t)\)</span> and the previous hidden-state <span class="math notranslate nohighlight">\(h(t-1)\)</span></p></li>
</ul>
<p>At each time stamp <span class="math notranslate nohighlight">\(t' \in [1+d,T+d]\)</span></p>
<ul class="simple">
<li><p>a new output-vector <span class="math notranslate nohighlight">\(y(t')\)</span> is calculated from the hidden-state <span class="math notranslate nohighlight">\(h(t')\)</span>. For <span class="math notranslate nohighlight">\(t'&gt;T\)</span>, the hidden-state <span class="math notranslate nohighlight">\(h(t')\)</span> is calculated only from <span class="math notranslate nohighlight">\(h(t'-1)\)</span>.</p></li>
</ul>
<p>The parameter <span class="math notranslate nohighlight">\(d\)</span> describes the delay between the input and the output sequence. In language translation usually <span class="math notranslate nohighlight">\(d&gt;0\)</span>, i.e the first word of the translation is calculated not before the first <span class="math notranslate nohighlight">\(d\)</span> words of the sentence in the source-language have been passed do the network.</p>
<p><img alt="manyToMany" src="http://maucher.home.hdm-stuttgart.de/Pics/manyToMany.PNG" /></p>
<p>The case, where <span class="math notranslate nohighlight">\(d&gt;0\)</span> is called the asynchronous case (picture above). The synchronous many-to-many case is defined by <span class="math notranslate nohighlight">\(d=0\)</span> (picture below). A application of this category is e.g. frame-by-frame labeling of a video-sequence.</p>
<p><img alt="manyToManySync" src="http://maucher.home.hdm-stuttgart.de/Pics/manyToManySync.PNG" /></p>
</div>
<div class="section" id="one-to-many-rnn">
<h4><span class="section-number">10.2.2.1.4. </span>One-To-Many (RNN)<a class="headerlink" href="#one-to-many-rnn" title="Permalink to this headline">¶</a></h4>
<p>Finally, the one-to-many recurrent architecture maps a single vector at the input, to a sequence of output-vectors. A concrete application of this category is image-captioning, where the input is a single image, and the output is a sequence of words, which describes the contents of the image in a natural language. As can be seen in the picture below, the input <span class="math notranslate nohighlight">\(x(t=1)\)</span> is applied to calculate the first hidden-state <span class="math notranslate nohighlight">\(h(t=1)\)</span>. All following hidden states <span class="math notranslate nohighlight">\(h(t)\)</span> are calculated only from <span class="math notranslate nohighlight">\(h(t-1)\)</span>. For each hidden-state <span class="math notranslate nohighlight">\(h(t)\)</span> a corresponding output <span class="math notranslate nohighlight">\(y(t)\)</span> is calculated.
<img alt="oneToMany" src="http://maucher.home.hdm-stuttgart.de/Pics/oneToMany.PNG" /></p>
</div>
<div class="section" id="bidirectional-rnn">
<h4><span class="section-number">10.2.2.1.5. </span>Bidirectional RNN<a class="headerlink" href="#bidirectional-rnn" title="Permalink to this headline">¶</a></h4>
<p>A bidirectional RNN calculates two types of hidden-states at it’s output:</p>
<ul class="simple">
<li><p>The first type is the same as described up to now in this notebook. I.e. the current recurrent hidden layer output <span class="math notranslate nohighlight">\(h^1(t)\)</span> is calculated from the current input <span class="math notranslate nohighlight">\(x(t)\)</span> and the recurrent hidden output from the previous time-step <span class="math notranslate nohighlight">\(h^1(t-1)\)</span>.</p></li>
<li><p>The second type calculates recurrent hidden states in the backward direction. I.e the current recurrent hidden layer output <span class="math notranslate nohighlight">\(h'^1(t)\)</span> is calculated from the current input <span class="math notranslate nohighlight">\(x(t)\)</span> and the recurrent hidden output from the next time-step <span class="math notranslate nohighlight">\(h'^1(t+1)\)</span>.</p></li>
</ul>
<p><img alt="bidirectionalRNN" src="http://maucher.home.hdm-stuttgart.de/Pics/biDirectionalRNN.png" /></p>
<p>The output of the network is then calculated from both hidden states <span class="math notranslate nohighlight">\(h^1(t)\)</span> and <span class="math notranslate nohighlight">\(h'^1(t)\)</span>. In this way not only dependencies from previous inputs, but also from future inputs is taken into account. This is particular helpful in Natural Language Processing tasks, where inputs are successive words.</p>
<p>Bidirectional RNNs are implemented by stacking together two RNN-layer modules, where each layer can be of any RNN-type, e.g. simple RNN, LSTM or GRU.</p>
<p><a id="lstm"></a></p>
</div>
</div>
</div>
<div class="section" id="long-short-term-memory-lstm-networks">
<h2><span class="section-number">10.2.3. </span>Long-short-Term-Memory (LSTM) networks<a class="headerlink" href="#long-short-term-memory-lstm-networks" title="Permalink to this headline">¶</a></h2>
<p>Simple RNNs, as described and depicted in <a class="reference external" href="#abstrnn">Simple RNN</a>, suffer from an important problem: Applying Stochastic Gradient Descent allows to learn short-term dependencies quite well, but the network is not capable to learn long-term dependencies. The reason for this drawback is the <a class="reference external" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient problem</a>: In SGD weights are updated proportional to the gradient of the loss-function with respect to the specific weight. In some areas of the networks these gradients get marginal, i.e. the weights are not adapted any longer. S. Hochreiter and J. Schmidhuber studied the problem of vanishing gradients in simple RNNs in their famous paper <a class="reference external" href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory">Long short-term Memory</a> and they proposed a new type of RNN, which circumvents this problem and is able to learn also long-term dependencies. The architecture of such a Long short-term Memory Network (LSTM) is depicted below:</p>
<p><img alt="lstm" src="http://maucher.home.hdm-stuttgart.de/Pics/abstrlstm.png" /></p>
<p>As shown in the figure above a LSTM keeps and updates a so called <em>cell-state</em>. This cell-state mainly realises the memory. The network is able to learn, which parts in the memory (cell-state) can be forgotten and which parts of the memory shall be updated. The new hidden-state <span class="math notranslate nohighlight">\(h^1(t)\)</span> depends not only on the previous hidden state <span class="math notranslate nohighlight">\(h^1(t-1)\)</span> and the current input <span class="math notranslate nohighlight">\(x(t)\)</span>, but also on the memory (cell-state).</p>
<p>The forget-gate outputs a filter <span class="math notranslate nohighlight">\(f(t)\)</span> which determines the information of the memory (cell-state), that can be forgotten. The second gate outputs <span class="math notranslate nohighlight">\(i(t)\)</span>, which determines which parts of the memory shall be updated and the output <span class="math notranslate nohighlight">\(\underline{C}(t)\)</span> determines how this update shall look like. The last gate at the bottom defines how the hidden-state shall be updated in dependance of the current input, the previous hidden state <span class="math notranslate nohighlight">\(h^1(t-1)\)</span> the new cell-state (memory).</p>
<p>Since LSTMs are a special kind of RNNs, all application-categories, described in subsection <a class="reference external" href="#appcat">RNN application categories</a> are also applicable for them.</p>
<p><a id="gru"></a></p>
</div>
<div class="section" id="gated-recurrent-unit-gru">
<h2><span class="section-number">10.2.4. </span>Gated Recurrent Unit (GRU)<a class="headerlink" href="#gated-recurrent-unit-gru" title="Permalink to this headline">¶</a></h2>
<p>Recurrent Neural Networks with Gated Recurrent Unit (GRU) have been introduced in <a class="reference external" href="https://arxiv.org/pdf/1409.1259.pdf">Cho et al: On the Properties of Neural Machine Translation: Encoder–Decoder Approaches</a>. They perform as well as LSTMs on sequence modelling, while  being conceptually simpler. As shown in the figure below, similar to the LSTM unit, the GRU has gating units that modulate the flow of information inside the unit, however, without having a separate cell-state memory. The output of the GRU <span class="math notranslate nohighlight">\(h^1(t)\)</span> is a linear interpolation of the previous output <span class="math notranslate nohighlight">\(h^1(t-1)\)</span> and a candidate output <span class="math notranslate nohighlight">\(\underline{h}^1(t)\)</span>. The weighting of these two elements is defined by the update-filter <span class="math notranslate nohighlight">\(i(t)\)</span>. The output <span class="math notranslate nohighlight">\(f(t)\)</span> of the forget-gate determines which parts from the previous hidden state <span class="math notranslate nohighlight">\(h^1(t-1)\)</span> can be forgotten in the calculation of the candidate output <span class="math notranslate nohighlight">\(\underline{h}^1(t)\)</span>.</p>
<p><img alt="abstrgru" src="http://maucher.home.hdm-stuttgart.de/Pics/abstrgru.png" /></p>
<p>An empirical evaluation of LSTMs and GRUs can be found in <a class="reference external" href="https://arxiv.org/pdf/1412.3555.pdf">Chung et al, Empirical Evaluation of Gated Recurrent Neural Networkson Sequence Modeling</a>.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./07neuralnetworks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="01NeuralNets.html" title="previous page"><span class="section-number">10.1. </span>Neural Networks Introduction</a>
    <a class='right-next' id="next-link" href="03ConvolutionNeuralNetworks.html" title="next page"><span class="section-number">10.3. </span>Convolutional Neural Networks</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>