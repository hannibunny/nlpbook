
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6.1. Latent Semantic Indexing (LSI) &#8212; Natural Language Processing Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6.2. Implementation of Topic Extraction and Document Clustering" href="02LatentSemanticIndexing.html" />
    <link rel="prev" title="6. Topic Extraction" href="05topicextraction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Natural Language Processing Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01access/01access.html">
   1. Access and Preprocess Text
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/01AccessTextFromFile.html">
     1.1. Access and Analyse Contents of Textfiles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/02crawlHTMLcontent.html">
     1.2. Access Contents of HTML Page
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/03crawlWebSites.html">
     1.3. Download HTML Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/04crawlRSSFeeds.html">
     1.4. Access RSS Feed
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/05RegularExpressions.html">
     1.5. Regular expressions in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/06accessTweets.html">
     1.6. Access Tweets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02normalisation/02normalisation.html">
   2. Word Normalisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/01morphology.html">
     2.1. Morphology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/03StemLemma.html">
     2.2. TextBlob Stemming and Lemmatization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/01LevenstheinDistance.html">
     2.3. Correction of Spelling Errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03postagging/03postagging.html">
   3. Part-Of-Speech Tagging
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03postagging/01tagsetsAndAlgorithms.html">
     3.1. PoS Tagsets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03postagging/02PosTagging.html">
     3.3. POS Tagging with NLTK
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04ngram/04ngram.html">
   4. N-Gram Language Model
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="05representationsintro.html">
   5. Vector Representations of Words and Documents
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="05representations.html">
     5.1. Vector Space Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02gensimDocModelSimple.html">
     5.3. Implementation of BoWs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="01WordEmbeddingImplementation.html">
     5.4. Implementation of Word-Embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="05topicextraction.html">
   6. Topic Extraction
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     6.1. Latent Semantic Indexing (LSI)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02LatentSemanticIndexing.html">
     6.2. Implementation of Topic Extraction and Document Clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06classification/06classification.html">
   7. Text Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/06classificationMetrics.html">
     7.1. Validation of Classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/07classificationNaiveBayes.html">
     7.2. Naive Bayes Text Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/FakeNewsClassification.html">
     7.3. Text Classification Application: Fake News detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07neuralnetworks/07neuralnetworks.html">
   8. Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/01NeuralNets.html">
     8.1. Neural Networks Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/02RecurrentNeuralNetworks.html">
     8.2. Recurrent Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/03ConvolutionNeuralNetworks.html">
     8.3. Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/04CNN.html">
     8.4. CNN, LSTM and Attention for IMDB Movie Review classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/attention.html">
     8.5. Sequence-To-Sequence, Attention, Transformer
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   9. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/05representations/05lsi.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="latent-semantic-indexing-lsi">
<h1><span class="section-number">6.1. </span>Latent Semantic Indexing (LSI)<a class="headerlink" href="#latent-semantic-indexing-lsi" title="Permalink to this headline">¶</a></h1>
<p>LSI has been developed in <span id="id1">[<a class="reference internal" href="../referenceSection.html#id4">DDF+90</a>]</span> as a method for topic-extraction. For the description of LSI, assume that we have 5 simple documents, which contain the follwing words:</p>
<ul class="simple">
<li><p>document d1: <em>cosmonaut, moon, car</em></p></li>
<li><p>document d2: <em>astronaut, moon</em></p></li>
<li><p>document d3: <em>cosmonaut</em></p></li>
<li><p>document d4: <em>car, truck</em></p></li>
<li><p>document d5: <em>car</em></p></li>
<li><p>document d6: <em>truck</em></p></li>
</ul>
<p>If we construct the BoW-matrix for these documents and transpose this matrix, the result is called <em>Inverted-Index</em>. In the example the Inverted Index is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(
\begin{array}{l|cccccc}
&amp; d1 &amp; d2 &amp; d3 &amp; d4 &amp; d5 &amp; d6 \\
\hline
cosmonaut &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
astronaut &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
moon &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
car &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\
truck &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\
\end{array}
\right)
\end{split}\]</div>
<p>Another representations of the form <span class="math notranslate nohighlight">\(\mathbf{w}=\mathbf{A}\cdot \mathbf{d}\)</span> is given below. The matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is called <strong>term-by-document matrix</strong>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(
\begin{array}{c}
cosmonaut \\
astronaut \\
moon \\
car \\
truck \\
\end{array}
\right)
=
\left(
\begin{array}{cccccc}
 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\
\end{array}
\right)
\cdot
\left(
\begin{array}{c}
d1 \\
d2 \\
d3 \\
d4 \\
d5 \\
d6 \\
\end{array}
\right)
\end{split}\]</div>
<p>In the Inverted-Index each column represents a document. Each column is an <span class="math notranslate nohighlight">\(t\)</span>-dimensional vector, where <span class="math notranslate nohighlight">\(t\)</span> is the number of different words in the corpus. Hence, each document can be considered as a point in the t-dimensional space. LSI defines a transformation from this <span class="math notranslate nohighlight">\(t\)</span>-dimensional space into a <span class="math notranslate nohighlight">\(k\)</span>-dimensional space, where <span class="math notranslate nohighlight">\(k\)</span> is usually much smaller than <span class="math notranslate nohighlight">\(t\)</span>. The representation of the documents in this new <span class="math notranslate nohighlight">\(k\)</span>-dimensional space is such that, documents which refer to the same topics, have similar k-dimensional vectors (are nearby points in the <span class="math notranslate nohighlight">\(k\)</span>-dimensional space).</p>
<p>For example the 2-dimensional space, the 6 documents of the example may have the following representations:<a class="footnote-reference brackets" href="#f1" id="id2">1</a></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left(
\begin{array}{l|cccccc}
&amp; d1 &amp; d2 &amp; d3 &amp; d4 &amp; d5 &amp; d6 \\
\hline
\{cosmonaut,astronaut,moon\} &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
\{car,truck\} &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
\end{array}
\right).
\end{split}\]</div>
<p>As can be seen in this 2-dimensional vector space document 2 and 3 are described by the same vector <span class="math notranslate nohighlight">\((1,0)\)</span>, even though in the original 6-dimensional space their vectors are orthogonal to each other, because these documents have no word in common.</p>
<p>As sketched in the matrix above, the new dimensions in the 2-dimensional space, do not belong to single words, but to topics and each topic is represented by a list of words, which frequently appear in the documents, which belong to this topic. More accurate: The new dimensions (topics) in the <span class="math notranslate nohighlight">\(k\)</span>-dimensional space are linear combinations of the old dimensions (words) in the <span class="math notranslate nohighlight">\(t\)</span>-dimensional space.</p>
<p>Note that this implies a different understanding of <em>semantically related words</em>, than the concept implied by Distributional Semantic Models (DSM). In DSMs two words are semantically related, if they frequently appear in the same context, where context is given by the surrounding words. <strong>In LSI two words are <em>semantically related</em>, if they frequently appear in documents with similar words</strong>.</p>
<p>Latent Semantic Indexing (LSI) applies <strong>Singular Value Decomposition (SVD)</strong> for calculating the low-dimensional topic space <span id="id3">[<a class="reference internal" href="../referenceSection.html#id3">MS00</a>]</span>. SVD calculates a factorisation of the term-by-document matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-svd-factors">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-svd-factors" title="Permalink to this equation">¶</a></span>\[
A_{t \times d} = T_{t \times n} S_{n \times n} (D_{d \times n})^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(d\)</span> are the number of words and the number of documents, respectively and <span class="math notranslate nohighlight">\(n=min(t,d)\)</span>. The factor matrices have the following properties:</p>
<ul class="simple">
<li><p>columns in <span class="math notranslate nohighlight">\(T\)</span> are orthonormal</p></li>
<li><p>columns in <span class="math notranslate nohighlight">\(D\)</span> are orthonormal</p></li>
<li><p>in <span class="math notranslate nohighlight">\(S\)</span> only elements on the main diagonal are non-zero.</p></li>
</ul>
<p>The elements on the main diagonal of <span class="math notranslate nohighlight">\(S\)</span> are the so called <em>Singular Values</em> in decreasing order. The Singular Values reflect the variance of data along the corresponding dimension. The 3 factor matrices define a rotation of the original space, such that</p>
<ul class="simple">
<li><p>the first dimension of the new space is defined by the direction, along which data varies maximal,</p></li>
<li><p>the second dimension of the new space is defined by the direction, which is orthogonal to the first, and belongs to the second strongest variance,</p></li>
<li><p>the third dimension of the new space is defined by the direction, which is orthogonal to the first two dimensions, and belongs to the third strongest variance,</p></li>
<li><p>…</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SVD can be considered to be a generalisation of Principal Component Analysis (PCA) in the sense that SVD can also be applied to non-square matrices. PCA calculates the Eigenvectors and Eigenvalues of the covariance-matrix of the given data. The Eigenvectors with the strongest associated Eigenvalues are the Principal Components, i.e. the directions, along which data-variance is maximal. Similar as Eigenvalues in PCA, the Singular Values of SVD belong to the directions of maximal data variance.</p>
</div>
<p>The SVD matrix factorisation applied to the term-by-document matrix of the example yields the following 3 factors:</p>
<div class="math notranslate nohighlight" id="equation-svd-t">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-svd-t" title="Permalink to this equation">¶</a></span>\[\begin{split}
T=
\left(
\begin{array}{l|ccccc}
&amp; dim1 &amp; dim2 &amp; dim3 &amp; dim4 &amp; dim5  \\
\hline
cosmonaut&amp;  0.44 &amp; -0.3 &amp; -0.57 &amp; 0.58 &amp; -0.25 \\
astronuat&amp;   0.13 &amp; -0.33 &amp; 0.59 &amp; -0.0 &amp; -0.73 \\
moon&amp;   0.48 &amp; -0.51  &amp; 0.37 &amp;-0.0 &amp;   0.61 \\
car&amp;   0.7 &amp;  0.35 &amp; -0.15 &amp;-0.58 &amp;-0.16 \\
truck&amp;   0.26 &amp; 0.65 &amp; 0.41 &amp; 0.58 &amp; 0.09 \\
 \end{array}
\right)
\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-svd-s">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-svd-s" title="Permalink to this equation">¶</a></span>\[\begin{split}
S=
\left(
\begin{array}{ccccc}
 2.16 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 1.59 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 1.28 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 1.00 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.39 \\
 \end{array}
\right)
\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-svd-d">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-svd-d" title="Permalink to this equation">¶</a></span>\[\begin{split}
D^T=
\left(
\begin{array}{l|cccccc}
&amp; d1 &amp; d2 &amp; d3 &amp; d4 &amp; d5 &amp; d6 \\
\hline
dim1 &amp; 0.75&amp; 0.28&amp; 0.2&amp;  0.45&amp; 0.33&amp; 0.12\\
dim2 &amp; -0.29&amp;-0.53&amp;-0.19&amp; 0.63&amp; 0.22&amp; 0.41\\
dim3 &amp; -0.28&amp; 0.75&amp;-0.45&amp; 0.2&amp; -0.12&amp; 0.33\\
dim4 &amp; -0.0&amp;   0.0&amp;   0.58&amp; 0.0 &amp; -0.58&amp; 0.58\\
dim5 &amp;  0.53 &amp;-0.29&amp;-0.63&amp; -0.19&amp;-0.41&amp; 0.22\\ 
  \end{array}
\right)
\end{split}\]</div>
<p>Given these factors, calculated by SVD, how do we obtain a lower-dimensional space from the original <span class="math notranslate nohighlight">\(n\)</span>-dimensional space, where <span class="math notranslate nohighlight">\(n\)</span> is the number of words in the corpus?</p>
<p>As indicated by the subscripts of the matrix-factors in <a class="reference internal" href="#equation-svd-factors">(6.1)</a></p>
<ul class="simple">
<li><p>matrix <span class="math notranslate nohighlight">\(T\)</span> has <span class="math notranslate nohighlight">\(n\)</span> columns</p></li>
<li><p>matrix <span class="math notranslate nohighlight">\(S\)</span> has <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns</p></li>
<li><p>matrix <span class="math notranslate nohighlight">\(D\)</span> has <span class="math notranslate nohighlight">\(n\)</span> columns</p></li>
</ul>
<p>The lower-dimensional space is now obtained by</p>
<ol>
<li><p>Select the number of dimensions <span class="math notranslate nohighlight">\(k\)</span> of the lower-dimensional space. Note that <span class="math notranslate nohighlight">\(k\)</span> is the number of topics, that shall be distinguished.</p></li>
<li><p>Keep the first <span class="math notranslate nohighlight">\(k\)</span> columns of matrix <span class="math notranslate nohighlight">\(T\)</span> and remove the other <span class="math notranslate nohighlight">\(n-k\)</span> columns to obtain the reduced matrix <span class="math notranslate nohighlight">\(T'\)</span>. Silmilarly, keep the first <span class="math notranslate nohighlight">\(k\)</span> rows and the first <span class="math notranslate nohighlight">\(k\)</span> columns of matrix <span class="math notranslate nohighlight">\(S\)</span> to obtain the reduced matrix <span class="math notranslate nohighlight">\(S'\)</span> and keep the first <span class="math notranslate nohighlight">\(k\)</span> columns of matrix <span class="math notranslate nohighlight">\(D\)</span> and to obtain the reduced matrix <span class="math notranslate nohighlight">\(D'\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-svd-tneu">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-svd-tneu" title="Permalink to this equation">¶</a></span>\[\begin{split}
	T'=
	\left(
	\begin{array}{l|cc}
	&amp; dim1 &amp; dim2  \\
	\hline
	cosmonaut&amp;  0.44 &amp; -0.3 \\
	astronaut&amp;   0.13 &amp; -0.33 \\
	moon&amp;   0.48 &amp; -0.51 \\
	car&amp;   0.7 &amp;  0.35 \\
	truck&amp;   0.26 &amp; 0.65  \\
	\end{array}
	\right)
	\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-svd-sneu">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-svd-sneu" title="Permalink to this equation">¶</a></span>\[\begin{split}
	S'=
	\left(
	\begin{array}{cc}
	 2.16 &amp; 0  \\
	 0 &amp; 1.59 \\
	 \end{array}
	\right)
	\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-svd-dneu">
<span class="eqno">(6.7)<a class="headerlink" href="#equation-svd-dneu" title="Permalink to this equation">¶</a></span>\[\begin{split}
	D'^T=
	\left(
	\begin{array}{l|cccccc}
	&amp; d1 &amp; d2 &amp; d3 &amp; d4 &amp; d5 &amp; d6 \\
	\hline
	dim1 &amp; 0.75&amp; 0.28&amp; 0.2&amp;  0.45&amp; 0.33&amp; 0.12\\
	dim2 &amp; -0.29&amp;-0.53&amp;-0.19&amp; 0.63&amp; 0.22&amp; 0.41\\
	\end{array}
	\right)
	\end{split}\]</div>
</li>
<li><p>Then</p>
<div class="math notranslate nohighlight">
\[
	A'=T' \cdot S' \cdot D'^T
	\]</div>
<p>is the best approximation, in terms of <em>least square error</em>, of the original term-by-document matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
</li>
<li><p>the columns of the <strong>Matrix <span class="math notranslate nohighlight">\(B=S' \cdot D'^T\)</span></strong> are the coordinates of the  documents in the <span class="math notranslate nohighlight">\(k-\)</span>dimensional <strong>latent semantic space</strong></p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-svd-b">
<span class="eqno">(6.8)<a class="headerlink" href="#equation-svd-b" title="Permalink to this equation">¶</a></span>\[\begin{split}
B=
\left(
\begin{array}{l|cccccc}
&amp; d1 &amp; d2 &amp; d3 &amp; d4 &amp; d5 &amp; d6 \\
\hline
dim1 &amp; 1.62 &amp; 0.60 &amp; 0.44&amp;  0.97&amp; 0.70&amp; 0.26\\
dim2 &amp; -0.46 &amp; -0.84 &amp;-0.3&amp; 1.00 &amp; 0.35 &amp; 0.65\\
\end{array}
\right)
\end{split}\]</div>
<ol class="simple">
<li><p><strong>A new document or query</strong> is first mapped to it’s BoW-vector <span class="math notranslate nohighlight">\(q\)</span>. Then the representation of this vector in the latent semantic space is calculated by</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
T'^T \cdot q^T.
\]</div>
<p>For example, if the query consists of the words <em>astronaut, moon</em> and <em>car</em>, then the corresponding BoW-vector is</p>
<div class="math notranslate nohighlight">
\[
q=(0,1,1,1,0)
\]</div>
<p>and the coordinates of this query in the latent semantic space are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
T'^T \cdot q^T = 
\left(
\begin{array}{c}
0.13+0.48+0.7 \\
-0.33-0.51+0.35	
\end{array}
\right)
=
\left(
\begin{array}{c}
1.31 \\
-0.49	
\end{array}
\right).
\end{split}\]</div>
<p>In the figure below, the 6 documents (columns of matrix <a class="reference internal" href="#equation-svd-b">(6.8)</a> ) and the new query are plotted in the latent semantic space.</p>
<figure align="center">
<img width="400" src="https://maucher.home.hdm-stuttgart.de/Pics/docsQueryIn2dimSpaceLSI.png">
<figcaption>Representations of documents and query vector in latent semantic space</figcaption>
</figure>
<p>As mentioned earlier document-vectors are often normed to unique-length. These normed vectors are shown in the figure below.</p>
<figure align="center">
<img width="400" src="https://maucher.home.hdm-stuttgart.de/Pics/docsQueryNormedIn2dimSpace.png">
<figcaption>Representations of normed documents and normed query vector in latent semantic space</figcaption>
</figure>
<p>As can be seen the documents belonging to the topic <em>vehicles</em> are located in an other region of the latent semantic space than the documents, which refer to the topic <em>space</em>. Moreover, the query-vector, which contains <em>space</em>-words is in the region of <em>space</em>-documents.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="f1"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Note that this representation is not the true result of the transformation, but a simplified one, just for desribing the idea of the approach.</p>
</dd>
</dl>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./05representations"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="05topicextraction.html" title="previous page"><span class="section-number">6. </span>Topic Extraction</a>
    <a class='right-next' id="next-link" href="02LatentSemanticIndexing.html" title="next page"><span class="section-number">6.2. </span>Implementation of Topic Extraction and Document Clustering</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>