
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5. Vector Representations of Words and Documents &#8212; Natural Language Processing Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6. Applying Word-Embeddings" href="01WordEmbeddingImplementation.html" />
    <link rel="prev" title="4. N-Gram Language Model" href="../04ngram/04ngram.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Natural Language Processing Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Introduction
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01access/01access.html">
   1. Access and Preprocess Text
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/01AccessTextFromFile.html">
     1.1. Access and Analyse Contents of Textfiles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/02crawlHTMLcontent.html">
     1.2. Access Contents of HTML Page
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/03crawlWebSites.html">
     1.3. Download HTML Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/04crawlRSSFeeds.html">
     1.4. Access RSS Feed
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/05RegularExpressions.html">
     1.5. Regular expressions in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01access/06accessTweets.html">
     1.6. Access Tweets
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02normalisation/02normalisation.html">
   2. Word Normalisation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/01morphology.html">
     2.1. Morphology
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/03StemLemma.html">
     2.2. TextBlob Stemming and Lemmatization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02normalisation/01LevenstheinDistance.html">
     2.3. Correction of Spelling Errors
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03postagging/03postagging.html">
   3. Part-Of-Speech Tagging
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03postagging/01tagsetsAndAlgorithms.html">
     3.1. PoS Tagsets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03postagging/02PosTagging.html">
     3.3. POS Tagging with NLTK
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04ngram/04ngram.html">
   4. N-Gram Language Model
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   5. Vector Representations of Words and Documents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01WordEmbeddingImplementation.html">
   6. Applying Word-Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02gensimDocModelSimple.html">
   7. Document models and similarity
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="05topicextraction.html">
   8. Topic Extraction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="05lsi.html">
     8.1. Latent Semantic Indexing (LSI)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02LatentSemanticIndexing.html">
     8.2. Implementation of Topic Extraction and Document Clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06classification/06classification.html">
   9. Text Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/06classificationMetrics.html">
     9.1. Validation of Classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/07classificationNaiveBayes.html">
     9.2. Naive Bayes Text Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06classification/FakeNewsClassification.html">
     9.3. Text Classification Application: Fake News detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07neuralnetworks/07neuralnetworks.html">
   10. Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/01NeuralNets.html">
     10.1. Neural Networks Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/02RecurrentNeuralNetworks.html">
     10.2. Recurrent Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/03ConvolutionNeuralNetworks.html">
     10.3. Convolutional Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07neuralnetworks/04CNN.html">
     10.4. CNN, LSTM and Attention for IMDB Movie Review classification
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   11. References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/05representations/05representations.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vector-space-model">
   5.1. Vector Space Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-word-model">
     5.1.1. Bag-of-Word Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-word-variants">
     5.1.2. Bag-of-Word Variants
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#one-hot-encoding-of-words">
     5.1.3. One-Hot-Encoding of Words
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bow-based-document-similarity">
     5.1.4. BoW-based document similarity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bow-drawbacks">
     5.1.5. BoW Drawbacks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#distributional-semantic-models">
   5.2. Distributional Semantic Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#count-based-dsm">
     5.2.1. Count-based DSM
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variants-of-count-based-dsms">
       5.2.1.1. Variants of count-based DSMs
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction-based-dsm">
     5.2.2. Prediction-based DSM
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#continous-bag-of-words-cbow">
       5.2.2.1. Continous Bag-Of-Words (CBOW)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#skip-gram">
       5.2.2.2. Skip-Gram
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#glove">
       5.2.2.3. GloVe
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fasttext">
       5.2.2.4. FastText
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dsm-downstream-tasks">
     5.2.3. DSM Downstream tasks
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="vector-representations-of-words-and-documents">
<h1><span class="section-number">5. </span>Vector Representations of Words and Documents<a class="headerlink" href="#vector-representations-of-words-and-documents" title="Permalink to this headline">¶</a></h1>
<p>As described in the <a class="reference internal" href="../intro.html"><span class="doc">Introduction</span></a>, many NLP tasks can either be solved in a rule-based or in a data-based approach. Data-based approaches increasingly yield better results than rule-based approaches. At the heart of data-based approaches is a Machine Learning algorithm, which learns a model from available training data. This model can then be applied e.g. for Document Classification, Sentiment Analysis, Named-Entity-Recognition, Intent-Recognition, Automatic translation and many other NLP tasks.</p>
<p>All ML-algorithms require a numeric representation at their input, usually a fixed-length numeric vector. In the context of NLP the input is usually text. The crucial question is then:</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question:</p>
<p>How to represent texts, i.e. sequences of words, punctuation marks…, as a numeric vector of constant length?</p>
</div>
<p>In this section first the general notion of <strong>Vector Space Model</strong> is introduced. Then the most common type of vector space model for text, the so called <strong>Bag-Of-Word (PoW)</strong> model is described. In the extreme case of single-word-texts the BoW melts down to the <strong>One-Hot-Encoding</strong> of words. The pros and cons of these conventional representations are discussed. Another method to represent words as numerical vectors is constituted by <strong>Distributional Semantic Models (DSMs)</strong>. The currently very popular <strong>Word Embeddings</strong> belong to the class of DSMs. <strong>Word Embeddings</strong> are numerical word representations, which are learned by <em>Neural Networks</em>. Even though they have been considered before, the 2013 milestone paper <span id="id1">[<a class="reference internal" href="../referenceSection.html#id6">MSC+13</a>]</span> introduced two very efficient methods to learn meaningful Word Embeddings. Since then Word Embeddings have revolutionized NLP.</p>
<div class="section" id="vector-space-model">
<h2><span class="section-number">5.1. </span>Vector Space Model<a class="headerlink" href="#vector-space-model" title="Permalink to this headline">¶</a></h2>
<p>Vector Space Models map arbitrary inputs to numeric vectors of fixed length. For a given task, you are free to define a set of <span class="math notranslate nohighlight">\(N\)</span> relevant features, which can be extracted from the input. Each of the <span class="math notranslate nohighlight">\(N\)</span>-feature extraction functions returns how often the corresponding feature appears in the input. Each component of the vector-representation belongs to one feature and the value at this component is the count of this feature in the input.</p>
<div class="dropdown admonition">
<p class="admonition-title">Example: Vector Space Model in General</p>
<p>Assume that your task is to classify texts into the classes <em>poetry</em> and <em>scientific paper</em>. The classifier shall be learned by a Machine Learning Algorithm which requires fixed-length numeric vectors at it’s input. You think about relevant features and come to the conclusion, that</p>
<ol class="simple">
<li><p>the average length of sentences (in words)</p></li>
<li><p>the number of proper names</p></li>
<li><p>the number of adjectives</p></li>
</ol>
<p>may be relevant features. Then all input-texts, independent of their length can be mapped to vectors of length <span class="math notranslate nohighlight">\(N=3\)</span>, whose components are the frequencies of this features in the text. E.g. the text</p>
<p><code class="docutils literal notranslate"><span class="pre">Mary</span> <span class="pre">loves</span> <span class="pre">the</span> <span class="pre">kind</span> <span class="pre">and</span> <span class="pre">handsome</span> <span class="pre">little</span> <span class="pre">boy.</span> <span class="pre">His</span> <span class="pre">name</span> <span class="pre">is</span> <span class="pre">Peter</span> <span class="pre">and</span> <span class="pre">he</span> <span class="pre">lived</span> <span class="pre">next</span> <span class="pre">door</span> <span class="pre">to</span> <span class="pre">Mary's</span> <span class="pre">jealous</span> <span class="pre">friend</span> <span class="pre">Anne.</span></code></p>
<p>maps into the vector</p>
<div class="math notranslate nohighlight">
\[
(11,4,4) 
\]</div>
</div>
<div class="section" id="bag-of-word-model">
<h3><span class="section-number">5.1.1. </span>Bag-of-Word Model<a class="headerlink" href="#bag-of-word-model" title="Permalink to this headline">¶</a></h3>
<p>In the general case the vector space model implies a vector, whose components are the frequencies of pre-defined features in the given input. In the special case of text (=documents), a vector space model is applied, where the features are defined to be all words of the vocabulary <span class="math notranslate nohighlight">\(V\)</span>. I.e. each component in the resulting vector corresponds to a word <span class="math notranslate nohighlight">\(w \in V\)</span> and the value of the component is the frequency of this word in the given document. This vector space model for texts is the so called <strong>Bag of Words (BoW)</strong> model and the frequency of a word in a given document is denoted <strong>term-frequency</strong>. Accordingly a set of documents is modelled by a <strong>Bag of Words matrix</strong>, whose rows belong to documents and whose columns belong to words.</p>
<div class="dropdown admonition">
<p class="admonition-title">Example: Bag of Word matrix</p>
<p>Assume, that the given playground-corpus contains only two documents</p>
<ul class="simple">
<li><p>Document 1: <em>not all kids stay at home</em></p></li>
<li><p>Document 2: <em>all boys and girls stay not at home</em></p></li>
</ul>
<p>The BoW model of these documents is then</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>all</p></th>
<th class="head"><p>and</p></th>
<th class="head"><p>at</p></th>
<th class="head"><p>boys</p></th>
<th class="head"><p>girls</p></th>
<th class="head"><p>home</p></th>
<th class="head"><p>kids</p></th>
<th class="head"><p>not</p></th>
<th class="head"><p>stay</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Document 1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Document 2</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<p>In this example the words in the matrix have been alphabetically ordered. This is not necessary.</p>
</div>
</div>
<div class="section" id="bag-of-word-variants">
<h3><span class="section-number">5.1.2. </span>Bag-of-Word Variants<a class="headerlink" href="#bag-of-word-variants" title="Permalink to this headline">¶</a></h3>
<p>The entries of the BoW-matrix, as introduced above, are the <strong>term-frequencies</strong>. I.e. the entry in row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(tf(i,j)\)</span> determines how often the term (word) of column <span class="math notranslate nohighlight">\(j\)</span>, appears in document <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>Another option is the <strong>binary BoW</strong>. Here, the binary entry in row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span> just indicates if term <span class="math notranslate nohighlight">\(j\)</span> appears in document <span class="math notranslate nohighlight">\(i\)</span>. The entry has value 1 if the term appears at least once, otherwise it is 0.</p>
<p><strong>TF-IDF BoW:</strong> The drawback of using term-frequency <span class="math notranslate nohighlight">\(tf(i,j)\)</span> as matrix entries is that all terms are weighted similarly, in particular rare words such as words with a <strong>strong semantic focus</strong> are weighted in the same way as very frequent words, such as articles. <em>TF-IDF</em> is a weighted term frequency (TF). The weights are the <em>inverse document frequencies (IDF)</em>. Actually, there are different definitions for the calculation of TF-IDF. A common definition is</p>
<div class="math notranslate nohighlight">
\[
\mbox{tf-idf}(i,j) = tf(i,j) \cdot \log(\frac{N}{df_j}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(tf(i,j)\)</span> is the frequency of term <span class="math notranslate nohighlight">\(j\)</span> in document <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(N\)</span> is the total number of documents and <span class="math notranslate nohighlight">\(df_j\)</span> is the number of documents, which contain term <span class="math notranslate nohighlight">\(j\)</span>. For words, which occure in all documents</p>
<div class="math notranslate nohighlight">
\[
\log(\frac{N}{df_j}) = \log(\frac{N}{N}) = 0,
\]</div>
<p>i.e. such words are disregarded in a BoW with TF-IDF entries. Otherwise, words with a very strong semantic focus usually appear in only a few documents. Then the small value of <span class="math notranslate nohighlight">\(df_j\)</span> yields a low <em>IDF</em>, i.e. the term-frequency of such a word is weighted strongly.</p>
</div>
<div class="section" id="one-hot-encoding-of-words">
<h3><span class="section-number">5.1.3. </span>One-Hot-Encoding of Words<a class="headerlink" href="#one-hot-encoding-of-words" title="Permalink to this headline">¶</a></h3>
<p>In the extreme case of <em>documents</em>, which contain only a single word, the corresponding <em>tf</em>-based BoW-vector, has only one component of value 1 (in the column, which belongs to this word), all other entries are zero. This is actually a common conventional numeric encoding of words, the so called <em>One-Hot-Encoding</em>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Example: One-Hot-Encoding of words</p>
<p>Assume, that the entire Vocabular is</p>
<div class="math notranslate nohighlight">
\[
V=(\mbox{all, and, at, boys, girls, home, kids, not, stay}).
\]</div>
<p>A possible One-Hot-Encoding of these words is then</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>all</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>and</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>at</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>boys</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>girls</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>home</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>kids</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>not</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>stay</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="bow-based-document-similarity">
<h3><span class="section-number">5.1.4. </span>BoW-based document similarity<a class="headerlink" href="#bow-based-document-similarity" title="Permalink to this headline">¶</a></h3>
<p>Numeric vector presentation of documents are not only required for Machine-Learning based NLP tasks. Another important application category is <strong>Information Retrieval (IR)</strong>. Information Retrieval deals with algorithms and models for searching information in large document collections. Web-search like <span class="xref myst">www.google.com</span> is only one example for IR. In such document-search applications the user defines a <strong>query</strong>, usually in terms of one or more words. The task of the IR system is then to</p>
<ul class="simple">
<li><p>return relevant documents, which match the query</p></li>
<li><p>rank the returned documents, such that the most important is at the top of the search-result</p></li>
</ul>
<p>Challenges in this context are:</p>
<ul class="simple">
<li><p>How to deduce what the user actually wants, given only a few query-words?</p></li>
<li><p>How to calculate the <strong>relevance</strong> of a document with respect to the query words?</p></li>
</ul>
<p>Question 1 will be addressed later further below, when <em>Distributional Semantic Models</em>, in particular <em>Word Embeddings</em> are introduced. The second question will be answered now.</p>
<p>The conventional approach for document search is to</p>
<ol class="simple">
<li><p>model all documents in the index as numerical vectors, e.g. by BoW</p></li>
<li><p>model the query as a numerical vector in the same way as the documents are modelled</p></li>
<li><p>determine the most relevant documents by just determining the document-vectors, which have the smallest distance to the query-vector.</p></li>
</ol>
<p>This means that the question on <em>relevance</em> is solved by determining <em>nearest vectors in a vector space</em>.
An example is given below. Here we assume, that there are only 3 documents in the index and there are only 4 different words, occuring in these documents. Document 2, for example, contains <em>word 1</em> with a frequency of 4 and <em>word 2</em> with a frequency of 5. The query consists of <em>word 1</em> and <em>word 2</em>. The BoW-matrix and the attached query-vector are:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="text-align:center head"><p>word 1</p></th>
<th class="text-align:center head"><p>word 2</p></th>
<th class="text-align:center head"><p>word 3</p></th>
<th class="text-align:center head"><p>word 4</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>document 1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>document 2</p></td>
<td class="text-align:center"><p>4</p></td>
<td class="text-align:center"><p>5</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
<tr class="row-even"><td><p>document 3</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Query</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Given these vector-representations, it is easy to determine the distances between each document and the query.</p>
<p>The obvious <em>type of distance</em> is the <strong>Euclidean Distance</strong>: For two vectors, <span class="math notranslate nohighlight">\(\underline{a}=(a_1,\ldots,a_n)\)</span> and <span class="math notranslate nohighlight">\(\underline{b}=(b_1,\ldots,b_n)\)</span>, the Euclidean Distance is defined to be</p>
<div class="math notranslate nohighlight">
\[
d_E(\underline{a},\underline{b})=\sqrt{\sum_{i=1}^n (a_i-b_i)^2}
\]</div>
<p><strong>Similarity</strong> and <strong>Distance</strong> are inverse to each other, i.e. the similarity between vectors increases with decreasing distance and vice versa. For each distance-measure a corresponding similarity-measure can be defined. E.g. the <em>Euclidean-distance</em>-based similarity measure is</p>
<div class="math notranslate nohighlight">
\[
s_E(\underline{a},\underline{b})=\frac{1}{1+d_E(\underline{a},\underline{b})}
\]</div>
<p>Now let’s determine the Euclidean distance between the query and the 3 documents in the example abover:</p>
<p>Euclidean distance between query and document 1:</p>
<div class="math notranslate nohighlight">
\[
d_E(\underline{q},\underline{d}_1)=\sqrt{(1-1)^2+(1-1)^2+(1-0)^2+(1-0)^2} = \sqrt{2} = 1.41
\]</div>
<p>Euclidean distance between query and document 2:</p>
<div class="math notranslate nohighlight">
\[
d_E(\underline{q},\underline{d}_2)=\sqrt{(4-1)^2+(5-1)^2+(0-0)^2+(0-0)^2} = \sqrt{25} = 5.00
\]</div>
<p>Euclidean distance between query and document 3:</p>
<div class="math notranslate nohighlight">
\[
d_E(\underline{q},\underline{d}_3)=\sqrt{(0-1)^2+(0-1)^2+(1-0)^2+(1-0)^2} = \sqrt{4} = 2.00
\]</div>
<p>Comparing these 3 distances, one can conclude, that document 1 has the smallest distance (and the highest similarity) to the query and is therefore the best match.</p>
<p><em>Is this what we expect?</em></p>
<p><em>No!</em> Document 2 contains the query words not only once but with a much higher frequency. One would expect, that this stronger prevalence of the query words implies that Document 2 is more relevant.</p>
<p><em>So what went wrong?</em></p>
<p>The answer is, that <strong>the Euclidean Distance is just the wrong distance-measure</strong> for this type of application. In a query each word is contained only once. Therefore, Euclidean-distance penalizes longer documents with more words.</p>
<p>The solution to this problem is</p>
<ul class="simple">
<li><p>either normalize all vectors - document vectors and query-vector - to unique length,</p></li>
<li><p>or apply another distance measure</p></li>
</ul>
<p>The standard similarity-measure for BoW vectors is the <strong>Cosine Similarity</strong>, which is calculated as defined in the table below. The table also contains the definition of the corresponding distance-measure. Moreover, a bunch of other distance- and similarity measures, which are frequently applied in NLP tasks, are listed in the table.</p>
<figure align="center">
<img width="700" src="https://maucher.home.hdm-stuttgart.de/Pics/distanceMeasures.png">
</figure> 
<p>For the query-example above, the Cosine-Similarities are:</p>
<p>Cosine Similarity between query and document 2:</p>
<div class="math notranslate nohighlight">
\[
s_C(\underline{q},\underline{d}_1)=\frac{1 \cdot 1 + 1 \cdot 1 + 1 \cdot 0 + 1 \cdot 0}{\sqrt{4} \cdot \sqrt{2}} = \frac{1}{\sqrt{2}} = 0.707
\]</div>
<p>Cosine Similarity between query and document 2:</p>
<div class="math notranslate nohighlight">
\[
s_C(\underline{q},\underline{d}_2)=\frac{4 \cdot 1 + 5 \cdot 1 + 0 \cdot 0 + 0 \cdot 0}{\sqrt{41} \cdot \sqrt{2}} = \frac{9}{\sqrt{82}} = 0.994
\]</div>
<p>Cosine Similarity between query and document 3:</p>
<div class="math notranslate nohighlight">
\[
s_C(\underline{q},\underline{d}_3)=\frac{0 \cdot 1 + 0 \cdot 1 + 1 \cdot 0 + 1 \cdot 0}{\sqrt{2} \cdot \sqrt{2}} = \frac{0}{2} = 0
\]</div>
<p>These calculated similarities match our subjective expectation: The similarity between document 3 and query q is 0 (the lowest possible value), since they have no word in common. The similarity between document 2 and the query q is close to the maximum similarity-value of 1, since both query-words appear with a high frequency in this document.</p>
</div>
<div class="section" id="bow-drawbacks">
<span id="bowdrawbacks"></span><h3><span class="section-number">5.1.5. </span>BoW Drawbacks<a class="headerlink" href="#bow-drawbacks" title="Permalink to this headline">¶</a></h3>
<p>BoW representation of documents and the One-Hot-Encoding of single words, as described above, are methods to map words and documents to numeric vectors, which can be applied as input for arbitrary Machine Learning algorithms. Hovever, these representations suffer from crucial drawbacks:</p>
<ol class="simple">
<li><p>The vectors are usually very long - there length is given by the number of words in the vocabulary. Moreover, the vectors are quite sparse, since the set of words appearing in one document is usually only a very small part of the set of all words in the vocabulary.</p></li>
<li><p>Semantic relations between words are not modelled. This means that in this model there is no information about the fact that word <em>car</em> is more related to word <em>vehicle</em> than to word <em>lake</em>.</p></li>
<li><p>In the BoW-model of documents word order is totally ignored. E.g. the model can not distinguish if word <em>not</em> appeared immediately before word <em>good</em> or before word <em>bad</em>.</p></li>
</ol>
<p>All of these drawbacks can be solved by applying <em>Distributional Semantic models</em> to map words into numeric vectors and by the way the resulting <em>Word Empeddings</em> are passed e.g. to the input of Recurrent Neural Networks, Convolutional Neural Networks or Transformers (see later chapters of this lecture).</p>
</div>
</div>
<div class="section" id="distributional-semantic-models">
<span id="dsm-label"></span><h2><span class="section-number">5.2. </span>Distributional Semantic Models<a class="headerlink" href="#distributional-semantic-models" title="Permalink to this headline">¶</a></h2>
<p>The linguistic theory of distributional semantics is based on the hypothesis, that words, which occur in similar contexts, have similar meaning. J.R. Firth formulated this assumption in his famous sentence <span id="id2">[<a class="reference internal" href="../referenceSection.html#id7">Fir57</a>]</span>:</p>
<p><em>You shall know a word by the company it keeps</em></p>
<p>Since computers can easily determine the co-occurrence statistics of words in large corpora the theory of distributional semantics provides a promising opportunity to automatically learn semantic relations. The learned semantic representations are called <strong>Distributional Semantic models (DSM)</strong>. <strong>They represent each word as a numerical vector, such that words, which appear frequently in similar contexts, are represented by similar vectors.</strong> In the figure below the arrow represents the DSM. Since there are many different DSMs there a many different approaches to implement this transformation from the hypothesis of distributional semantics to the word space.</p>
<figure align="center">
<img width="500" src="https://maucher.home.hdm-stuttgart.de/Pics/semanticsInWordSpace.png">
<figcaption>Mapping the hypothesis of distributional semantics to a word space</figcaption>
</figure>
<p>The field of DSMs can be categorized into the classes <strong>count-based models</strong> and <strong>prediction-based models</strong>. Recently, considerable attention has been focused on the question which of these classes is superior <span id="id3">[<a class="reference internal" href="../referenceSection.html#id8">PSM14</a>]</span>.</p>
<div class="section" id="count-based-dsm">
<h3><span class="section-number">5.2.1. </span>Count-based DSM<a class="headerlink" href="#count-based-dsm" title="Permalink to this headline">¶</a></h3>
<p>DSMs map words to numeric vectors, such that semantically related words, i.e. words which appear frequently in a similar context, have similar numeric vectors. The first question which arises from this definition is <em>What is context?</em> In all DSMs, introduced in this lecture, the context of a target word <span class="math notranslate nohighlight">\(w\)</span> is considered to be the sequence of <span class="math notranslate nohighlight">\(L\)</span> previous and <span class="math notranslate nohighlight">\(L\)</span> following words, where the <em>context-length</em> <span class="math notranslate nohighlight">\(L\)</span> is a parameter.</p>
<p>I.e. in the word-sequence</p>
<div class="math notranslate nohighlight">
\[
\ldots,w_{i-L},\ldots,w_{i-1},w_i,w_{i+1},\ldots,w_{i+L},\ldots
\]</div>
<p>the words <span class="math notranslate nohighlight">\(w_{i-L},\ldots,w_{i-1},w_{i+1},\ldots,w_{i+L}\)</span> constitute the context of target word <span class="math notranslate nohighlight">\(w_i\)</span> and</p>
<div class="math notranslate nohighlight">
\[
\left\{(w_i,w_{i+j})\right\}, \, j \in \{-L,\ldots,-1,1,\ldots,L\}
\]</div>
<p>is the corresponding <em>set of word-context-pairs</em> w.r.t. target word <span class="math notranslate nohighlight">\(w_i\)</span>.</p>
<p>The most common numeric vector representation of count-based DSMs can be derived from the <strong>Word-Co-Occurence</strong> matrix. In this matrix each row belongs to a target-word and each column belongs to a context word. Hence, the matrix has usually <span class="math notranslate nohighlight">\(\mid V \mid\)</span> rows and the same amount of columns<a class="footnote-reference brackets" href="#f1" id="id4">1</a>. In this matrix the entry in row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span> is the number of times context word <span class="math notranslate nohighlight">\(c_j\)</span> appears in the context of target word <span class="math notranslate nohighlight">\(w_i\)</span>. This frequency is denoted by <span class="math notranslate nohighlight">\(\#(w_i,c_j)\)</span>. The structure of such a word-co-occurence matrix is given below:</p>
<figure align="center">
<img width="600" src="https://maucher.home.hdm-stuttgart.de/Pics/cooccurenceMatrix.png">
<figcaption>Word-Cooccurence-Matrix</figcaption>
</figure>
<p>In order to determine this matrix a large corpus of contigous text is required. Then for all target-context pairs the corresponding count <span class="math notranslate nohighlight">\(\#(w_i,c_j)\)</span> must be determined. <strong>Once the matrix is complete the numeric vector representation of word <span class="math notranslate nohighlight">\(w_i\)</span> is just the i.th row in this matrix!</strong>.</p>
<div class="dropdown admonition">
<p class="admonition-title">Example: Word-Co-Occurence Matrix</p>
<p>Assume that the (unrealistically small) corpus is</p>
<p>K = [The dusty road ends nowhere. The dusty track ends there.*</p>
<p>For a (unrealistically small) context length of <span class="math notranslate nohighlight">\(L=2\)</span> the word-co-occurence matrix is:</p>
<figure align="center">
<img width="600" src="https://maucher.home.hdm-stuttgart.de/Pics/cooccurenceMatrixExample.png">
<figcaption>Example of Word-Cooccurence-Matrix</figcaption>
</figure>
<p>With this matrix, the numeric vector representation of word <strong>road</strong> is:</p>
<div class="math notranslate nohighlight">
\[
(1,1,0,1,1,0,0)
\]</div>
<p>and the vector for word <strong>track</strong> is:</p>
<div class="math notranslate nohighlight">
\[
(1,1,0,1,0,0,1)
\]</div>
<p>As can be seen, these two vectors are quite similar. The reason for this similarity is that both words appear in similar contexts. Hence we assume that they are semantically correlated.</p>
</div>
<p>With respect to the drawbacks of BoW-vectors as mentioned in subsection <a class="reference internal" href="#bowdrawbacks"><span class="std std-ref">BoW Drawbacks</span></a>, the count-based DSM-vectors provide the important advantage of modelling semantic relations between words: Pairs of semantically related words are closer to each other, than unrelated words. However, the count-based DSM vectors, as introduced so far, still suffer from the drawback of <em>long and sparse vectors</em>. This drawback can be eliminated by applying a <strong>dimensionality reduction</strong> such as <em>Principal Component Analysis (PCA)</em> or <em>Singular Value Decomposition (SVD)</em>, which are introduced later on in this lecture.</p>
<div class="section" id="variants-of-count-based-dsms">
<h4><span class="section-number">5.2.1.1. </span>Variants of count-based DSMs<a class="headerlink" href="#variants-of-count-based-dsms" title="Permalink to this headline">¶</a></h4>
<p>Count-based DSMs differ in the following parameters:</p>
<ul>
<li><p><strong>Context-Modelling:</strong> In any case the rows in the word-context
matrix are the vector-representation of the corresponding word, i.e.
each row uniquely corresponds to a word. The columns describe the
context, but different types of context can be considered, e.g. the
context can be defined by the previous words or the previous and following words.
Moreover, the <strong>window-size</strong>, which
defines the number of surrounding words, which are considered to be
within the context is an important parameter.</p></li>
<li><p><strong>Preprocessing:</strong> Different types of preprocessing, e.g.
stop-word-filtering, high-frequency cut-off, normalisation,
lemmatization can be applied to the given corpus. Different
preprocessing techniques yield different word-context matrices.</p></li>
<li><p><strong>Weighting Scheme:</strong> The entries <span class="math notranslate nohighlight">\(w_{ij}\)</span> in the word-context
matrix somehow measure the association between word <span class="math notranslate nohighlight">\(i\)</span> and context
<span class="math notranslate nohighlight">\(j\)</span>. In the most simple case <span class="math notranslate nohighlight">\(w_{ij}\)</span> is just the frequency of
context <span class="math notranslate nohighlight">\(j\)</span> in the context of word <span class="math notranslate nohighlight">\(i\)</span>. However, many different
alternatives for defining the entries in the <em>word-co-occuruence-matrix</em> exist. For example</p>
<ul class="simple">
<li><p>the <strong>conditional probability</strong> <span class="math notranslate nohighlight">\(P(c_j|w_i)\)</span>, which is determined by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    P(c_j|w_i)=\frac{\#(w_i,c_j)}{\#(w_i)},
    \]</div>
<ul class="simple">
<li><p>the <strong>pointwise mutual information (PMI)</strong> of <span class="math notranslate nohighlight">\(w_i\)</span> and <span class="math notranslate nohighlight">\(c_j\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
	PMI(w_i,c_j)=\log_2 \frac{P(w_i,c_j)}{P(w_i)P(c_j)} = \log_2 \frac{\#(w_i,c_j) |D|}{\#(w_i) \#(c_j)},
	\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the training corpus and <span class="math notranslate nohighlight">\(\mid D \mid\)</span> is the number of words in this corpus,</p>
<ul>
<li><p>the <strong>positive pointwise mutual information (PPMI)</strong>:</p>
<div class="math notranslate nohighlight">
\[
      PPMI(w_i,c_j) = \max\left(PMI(w_i,c_j),0 \right).
      \]</div>
</li>
</ul>
</li>
<li><p><strong>Dimensionality Reduction:</strong> Depending on the definition of context
the word vectors may be very sparse. Transforming these sparse
vectors to a lower dimensional space yields a lower complexity, but
may also yield better generalisation of the model. Different
dimensionality reduction schemes such as PCA, SVD or autoencoder yield different
word-vector spaces.</p></li>
<li><p><strong>Size of word vectors:</strong> The size of the word vectors depend on
context modelling, preprocessing and dimensionality reduction.</p></li>
<li><p><strong>Similarity-/Distance-Measure</strong>. In count-based word-spaces
different metrics to measure similarity between the word vectors can
be applied, e.g. cosine-similarity or Hellinger distance. The
performance of the word-space model strongly depends on the applied
similarity measure.</p></li>
</ul>
</div>
</div>
<div class="section" id="prediction-based-dsm">
<h3><span class="section-number">5.2.2. </span>Prediction-based DSM<a class="headerlink" href="#prediction-based-dsm" title="Permalink to this headline">¶</a></h3>
<p>In 2013 Mikolov et al. published their milestone-paper <em>Efficient Estimation of Word Representations in Vector Space</em> <span id="id5">[<a class="reference internal" href="../referenceSection.html#id6">MSC+13</a>]</span>. They proposed quite simple neural network architectures to efficiently create DSM word-embeddings: CBOW and Skipgram. These architectures are better known as <strong>Word2Vec</strong>. In both techniques neural networks are trained for a pseudo-task. After training, the network itself is usually not of interest. However, the learned weights in the input-layer constitute the word-embeddings, which can then be applied for a large field of NLP-tasks, e.g. document classification.</p>
<div class="section" id="continous-bag-of-words-cbow">
<h4><span class="section-number">5.2.2.1. </span>Continous Bag-Of-Words (CBOW)<a class="headerlink" href="#continous-bag-of-words-cbow" title="Permalink to this headline">¶</a></h4>
<p>The idea of CBOW is to predict the target word <span class="math notranslate nohighlight">\(w_i\)</span>, given the <span class="math notranslate nohighlight">\(N\)</span> context-words <span class="math notranslate nohighlight">\(w_{i-N/2},\ldots, w_{i-1}, \quad w_{i+1}, w_{i+N/2}\)</span>.
In order to learn such a predictor a large but unlabeled corpus is required. The extraction of training-samples from a corpus is sketched in the picture below:</p>
<figure align="center">
<img width="600" src="https://maucher.home.hdm-stuttgart.de/Pics/CBowTrainSamples.png">
<figcaption>CBOW Training Data</figcaption>
</figure>
<p>In this example a context length of <span class="math notranslate nohighlight">\(N=4\)</span> has been applied. The first training-element consists of</p>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(N=4\)</span> input-words <em>(happy,families,all,alike)</em></p></li>
<li><p>the target word <em>are</em>.</p></li>
</ul>
<p>In order to obtain the second training-sample the window of length <span class="math notranslate nohighlight">\(N+1\)</span> is just shifted by one to the right. The concrete architecture for CBOW is shown in the picture below. At the input the <span class="math notranslate nohighlight">\(N\)</span> context words are one-hot-encoded. The fully-connected <em>Projection-layer</em> maps the context words to a vector representation of the context. This vector representation is the input of a softmax-output-layer. The output-layer has as much neurons as there are words in the vocabulary <span class="math notranslate nohighlight">\(V\)</span>. Each neurons uniquely corresponds to a word of the vocabulary and outputs an estimation of the probaility, that the word appears as target for the current context-words at the input.</p>
<figure align="center">
<img width="600" src="https://maucher.home.hdm-stuttgart.de/Pics/cbowGramArchitecture.png">
<figcaption>CBOW: Neural Network is trained to predict likelihoods of possible target words, given the context word at the input of the network. At the input the context words are represented in One-Hot-encoder form. The learned word-vector is made up of the weights of the connections from the one-hot-encoded word to the Linear Projection Layer</figcaption>
</figure>
<p>After training the CBOW-network the vector representation of word <span class="math notranslate nohighlight">\(w\)</span> are the weights from the one-hot encoded word <span class="math notranslate nohighlight">\(w\)</span> at the input of the network to the neurons in the projection-layer. I.e. the number of neurons in the projection layer define the length of the word-embedding.</p>
</div>
<div class="section" id="skip-gram">
<h4><span class="section-number">5.2.2.2. </span>Skip-Gram<a class="headerlink" href="#skip-gram" title="Permalink to this headline">¶</a></h4>
<p>Skip-Gram is similar to CBOW, but has a reversed prediction process: For a given target word at the input, the Skip-Gram model predicts words, which are likely in the context of this target word. Again, the context is defined by the <span class="math notranslate nohighlight">\(N\)</span> neighbouring words. The extraction of training-samples from a corpus is sketched in the picture below:</p>
<figure align="center">
<img width="600" src="https://maucher.home.hdm-stuttgart.de/Pics/skipGramTrainSamples.png">
<figcaption>Skipgram Training Data</figcaption>
</figure>
<p>Again a context length of <span class="math notranslate nohighlight">\(N=4\)</span> has been applied. The first training-element consists of</p>
<ul class="simple">
<li><p>the first target word <em>(happy)</em> as input to the network</p></li>
<li><p>the first context word <em>(families)</em> as network-output.</p></li>
</ul>
<p>The concrete architecture for Skip-gram is shown in the picture below. At the input the target-word is one-hot-encoded. The fully-connected <em>Projection-layer</em> outputs the current vector representation of the target-word. This vector representation is the input of a softmax-output-layer. The output-layer has as much neurons as there are words in the vocabulary <span class="math notranslate nohighlight">\(V\)</span>. Each neurons uniquely corresponds to a word of the vocabulary and outputs an estimation of the probaility, that the word appears in the context of the current target-word at the input.</p>
<figure align="center">
<img width="600" src="https://maucher.home.hdm-stuttgart.de/Pics/skipGramArchitecture.png">
<figcaption>Skipgram: Neural Network is trained to predict likelihoods of possible context words, given the target word at the input of the network. At the input the target word word is represented in One-Hot-encoder form. The learned word-vector is made up of the weights of the connections from the one-hot-encoded word to the Linear Projection Layer</figcaption>
</figure>
</div>
<div class="section" id="glove">
<h4><span class="section-number">5.2.2.3. </span>GloVe<a class="headerlink" href="#glove" title="Permalink to this headline">¶</a></h4>
<p>In light of the two different approaches of DSMs, count-based and prediction-based models, Pennington et al developed in <span id="id6">[<a class="reference internal" href="../referenceSection.html#id8">PSM14</a>]</span> an approach called <em>Global Vectors</em>, which claims to combine the advantages of both DSM types. The advantage of count-based models is that they capture global co-occurrence statistics. Prediction based models do not operate on the global co-occurence statistics, but scan context windows across the entire corpus. On the other hand prediction-based models demonstrated in many evaluations that they are capable to learn linguistic patterns as linear relationships between the word vectors, indicating a vector space structure, which reflects linguistic semantics.</p>
<p>GloVe integrates the advantages of both approaches by minimizing a loss function, which is a weighted least-square function, that contains the difference between the scalar-product of a word vector <span class="math notranslate nohighlight">\(w_i\)</span> and a context vector <span class="math notranslate nohighlight">\(\tilde{w}_j\)</span> and the logarithm of the word-cooccruence-matrix-entry <span class="math notranslate nohighlight">\(\#(w_i,c_j)\)</span> (see <span id="id7">[<a class="reference internal" href="../referenceSection.html#id8">PSM14</a>]</span> for more details).</p>
</div>
<div class="section" id="fasttext">
<h4><span class="section-number">5.2.2.4. </span>FastText<a class="headerlink" href="#fasttext" title="Permalink to this headline">¶</a></h4>
<p>Another word embedding model is <strong>fastText</strong> from Facebook, which was introduced in 2017 <span id="id8">[<a class="reference internal" href="../referenceSection.html#id9">BGJM16</a>]</span> <a class="footnote-reference brackets" href="#f2" id="id9">2</a>. fastText is based on the Skipgram architecture, but instead using at the input the words itself, it applies character-sequences of length <span class="math notranslate nohighlight">\(n\)</span> (n-grams on character-level). For example for $n=3, the word <em>fastText</em> would be presented as the following set of character-level 3-grams:</p>
<div class="math notranslate nohighlight">
\[
fas, ast, stT, tTe, Tex, ext 
\]</div>
<p>The advantages of this approach over word embeddings, that work on word level.</p>
<ul class="simple">
<li><p>The morphology of words is taken into account, which is especially important in languages with large vocabularies and many rare words - such as German.</p></li>
<li><p>Prefixes, suffixes and compound words can be better understood</p></li>
<li><p>for words, which appear not in the training, it is likely that sub-n-grams are available</p></li>
</ul>
</div>
</div>
<div class="section" id="dsm-downstream-tasks">
<h3><span class="section-number">5.2.3. </span>DSM Downstream tasks<a class="headerlink" href="#dsm-downstream-tasks" title="Permalink to this headline">¶</a></h3>
<p>By applying DSM one can reliably determine for a given query word a set of semantically or syntactically related word by nearest-neighbour search in a vector space. In the picture below (Source: <span id="id10">[<a class="reference internal" href="../referenceSection.html#id11">CWB+11</a>]</span>) for some query words (in the topmost row of the table), the words, whose vectors are closest to the vector of the query-word are listed. For example, the word-vectors, which are closest to the vector of <em>France</em> are: <em>Austria, Belgium, Germany, …</em></p>
<figure align="center">
<img width="600" src="https://maucher.home.hdm-stuttgart.de/Pics/wordNeighbours.PNG">
<figcaption>Each column contains the nearest neighbours of the topmos word of the column</figcaption>
</figure>
<p>Moreover, as shown in the picture below (Source: <span id="id11">[<a class="reference internal" href="../referenceSection.html#id6">MSC+13</a>]</span>), also semantic relations between pairs of words can be determined, by subtracting one vector from the other. For example subtracting the vector for <em>man</em> from the vector of <em>woman</em> is another vector, which represents the <strong>female-male-relation</strong>. Adding this vector to the vector of <em>king</em> results in the vector of <em>queen</em>. Hence word embeddings can solve questions like <em>men ist to woman as queen is to?</em> <a class="footnote-reference brackets" href="#f3" id="id12">3</a>.</p>
<figure align="center">
<img width="600" src="https://maucher.home.hdm-stuttgart.de/Pics/wordRelations.PNG">
<figcaption>Prediction-based DSM are also able to model relations.</figcaption>
</figure>
<p>The ability to model semantic similarity and semantic relations has made Word-Embedings an essential building block for many NLP applications (<em>downstream tasks</em>) <span id="id13">[<a class="reference internal" href="../referenceSection.html#id10">Bak18</a>]</span>, e.g.</p>
<ul class="simple">
<li><p>Noun Phrase Chunking</p></li>
<li><p>Named Entity Recognition (NER)</p></li>
<li><p>Sentiment Analysis: Determine sentiment in sentences and text</p></li>
<li><p>Syntax Parsing: Determine the syntax tree of a sentence</p></li>
<li><p>Semantic Role Labeling (SRL): see e.g. <span id="id14">[<a class="reference internal" href="../referenceSection.html#id2">JM09</a>]</span> for a definition of semantic roles</p></li>
<li><p>Negation Scope Detection</p></li>
<li><p>POS-Tagging</p></li>
<li><p>Text Classification</p></li>
<li><p>Metaphor Detection</p></li>
<li><p>Paraphrase Detection</p></li>
<li><p>Textual Entailment Detection: Determine if some a text-part is an entailment of another</p></li>
<li><p>Automatic Translation</p></li>
</ul>
<p>All in all, since their breakthrough in 2013 (<span id="id15">[<a class="reference internal" href="../referenceSection.html#id6">MSC+13</a>]</span>), prediction based Word-Embeddings have revolutionized many NLP applications.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="f1"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p>One may have different vocabularies for target-words (<span class="math notranslate nohighlight">\(V_W\)</span>) and context-words (<span class="math notranslate nohighlight">\(V_C\)</span>). However, often they coincide, i.e. <span class="math notranslate nohighlight">\(V = V_W=V_C\)</span>.</p>
</dd>
<dt class="label" id="f2"><span class="brackets"><a class="fn-backref" href="#id9">2</a></span></dt>
<dd><p>Actually fastText has been introduced already in 2016, but not as a Word-Embedding, but as a text classifier. In 2017 it has been adapted as a word-embedding.</p>
</dd>
<dt class="label" id="f3"><span class="brackets"><a class="fn-backref" href="#id12">3</a></span></dt>
<dd><p>It has been shown that such <em>relations</em> can be determined by prediction-based DSMs, but not with conventional count-based DSMs.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./05representations"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../04ngram/04ngram.html" title="previous page"><span class="section-number">4. </span>N-Gram Language Model</a>
    <a class='right-next' id="next-link" href="01WordEmbeddingImplementation.html" title="next page"><span class="section-number">6. </span>Applying Word-Embeddings</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>